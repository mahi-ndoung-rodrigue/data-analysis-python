{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3109ca6e-7638-4060-a992-290b2bff1474",
   "metadata": {},
   "source": [
    "Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c38cd-4901-44b9-8287-980e5bef09b0",
   "metadata": {},
   "source": [
    "L'analyse de données avec Python est une compétence essentielle pour les Data Scientists et les Data Analysts. Ce cours vous emmènera des bases de l'analyse de données avec Python à la construction et à l'évaluation de modèles de données. Les sujets abordés sont les suivants - la collecte et l'importation de données - le nettoyage, la préparation et le formatage des données - la manipulation des cadres de données - le résumé des données - la construction de modèles de régression d'apprentissage automatique - le raffinement des modèles - la création de pipelines de données Vous apprendrez à importer des données à partir de sources multiples, à nettoyer et à manipuler les données, à effectuer une analyse exploratoire des données (AED) et à créer des visualisations de données significatives. Vous prédirez ensuite les tendances futures à partir des données en développant des modèles de régression linéaire, multiple, polynomiale et des pipelines et apprendrez à les évaluer. En plus des conférences vidéo, vous apprendrez et pratiquerez en utilisant des laboratoires et des projets pratiques. Vous travaillerez avec plusieurs bibliothèques Python open source, y compris Pandas et Numpy pour charger, manipuler, analyser et visualiser des ensembles de données intéressantes. Vous travaillerez également avec scipy et scikit-learn, pour construire des modèles d'apprentissage automatique et faire des prédictions. Si vous choisissez de suivre ce cours et d'obtenir le certificat de coursera, vous obtiendrez également un badge numérique IBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d6058d-46f9-4924-a770-8744df3be7ec",
   "metadata": {},
   "source": [
    "# Comprendre les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714a09e8-dd4f-4b41-9687-127bf1eef8a9",
   "metadata": {},
   "source": [
    "nous allons examiner l'ensemble de données sur les prix des voitures d'occasion. L'ensemble de données utilisé dans ce cours est un ensemble de données ouvert de Jeffrey C Schlimmer. Cet ensemble de données est au format CSV, qui sépare chacune des valeurs par des virgules, ce qui facilite son importation dans la plupart des outils ou applications. Chaque ligne représente une ligne de l'ensemble de données. Dans l'atelier pratique de ce module, vous serez en mesure de télécharger et d'utiliser le fichier CSV. Avez-vous remarqué quelque chose de différent à propos de la première rangée ? Parfois, la première ligne est un en-tête qui contient un nom de colonne pour chacune des 26 colonnes. Mais dans cet exemple, il ne s'agit que d'une autre ligne de données. Voici donc la documentation sur ce que représente chacune des 26 colonnes. Il y a beaucoup de colonnes, et je vais juste passer en revue quelques-uns des noms de colonnes. Mais vous pouvez également consulter le lien au bas de la diapositive pour parcourir vous-même les descriptions. Le premier attribut, le symbole, correspond au niveau de risque d'assurance d'une voiture. Les voitures se voient initialement attribuer un symbole de facteur de risque associé à leur prix. Ensuite, si une automobile est plus risquée, ce symbole est ajusté en le déplaçant vers le haut de l'échelle. Une valeur de +3 indique que la voiture est risquée, -3, qu'elle est probablement assez sûre. Le deuxième attribut, les pertes normalisées, est le montant moyen relatif des indemnités pour sinistres par année de véhicule assuré. Cette valeur est normalisée pour toutes les voitures appartenant à une catégorie de taille donnée (deux portes, petites voitures, break, voitures de sport , etc.) et représente la perte moyenne par voiture et par an. Les valeurs sont comprises entre 65 et 256. Les autres attributs sont faciles à comprendre. Si vous souhaitez obtenir plus de détails, consultez le lien au bas de la diapositive. Bien, après avoir compris la signification de chaque fonctionnalité, nous remarquerons que le 26e attribut est le prix. Il s'agit de notre valeur ou de notre étiquette cible. En d'autres termes, cela signifie que le prix est la valeur que nous voulons prédire à partir de l'ensemble de données, et que les prédicteurs doivent être toutes les autres variables répertoriées, telles que la symbolisation, les pertes normalisées, le profit, etc. L'objectif de ce projet est donc de prévoir le prix en fonction des autres caractéristiques de la voiture. Juste une petite remarque, cet ensemble de données date en fait de 1985, de sorte que le prix des voitures pour les modèles peut sembler un peu bas. Mais n'oubliez pas que le but de cet exercice est d'apprendre à analyser les données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62109325-5a8e-4400-950e-c8df80e60752",
   "metadata": {},
   "source": [
    "# Packages Python pour la science des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef2612-3ec5-42d8-acf8-3790a1f18a1c",
   "metadata": {},
   "source": [
    "Pour effectuer une analyse de données en Python, nous devons d'abord vous parler un peu des principaux packages pertinents pour l'analyse en Python. Une bibliothèque Python est un ensemble de fonctions et de méthodes qui vous permettent d'effectuer de nombreuses actions sans écrire de code. Les bibliothèques contiennent généralement des modules intégrés fournissant différentes fonctionnalités que vous pouvez utiliser directement. Il existe de nombreuses bibliothèques offrant un large éventail de services. Nous avons divisé les bibliothèques d'analyse de données Python en trois groupes. Le premier groupe s'appelle Bibliothèques informatiques scientifiques. Pandas propose une structure de données et des outils pour une manipulation et une analyse efficaces des données. Il fournit un accès rapide aux données structurées. L'instrument principal de Pandas est un tableau bidimensionnel composé d'étiquettes de colonnes et de lignes, appelées trame de données. Il est conçu pour fournir une fonctionnalité d'indexation facile. La bibliothèque NumPy utilise des tableaux pour ses entrées et sorties. Il peut être étendu aux objets pour les matrices, et avec des modifications de codage mineures, les développeurs peuvent effectuer un traitement rapide des tableaux. SciPy inclut des fonctions pour certains problèmes mathématiques avancés répertoriés sur cette diapositive, ainsi que pour la visualisation des données. L'utilisation de méthodes de visualisation des données est le meilleur moyen de communiquer avec les autres et de montrer les résultats significatifs de l'analyse. Ces bibliothèques vous permettent de créer des graphiques, des tableaux et des cartes. Le package Matplotlib lib est la bibliothèque la plus connue pour la visualisation de données. Il est idéal pour créer des graphiques et des diagrammes. Les graphiques sont également hautement personnalisables. Seaborn est une autre bibliothèque de visualisation de haut niveau. Il est basé sur Matplotlib. Il est très facile de générer différents diagrammes, tels que des cartes thermiques, des séries chronologiques et des diagrammes pour violons. Grâce aux algorithmes d'apprentissage automatique, nous sommes en mesure de développer un modèle à partir de notre ensemble de données et d'obtenir des prédictions. Les bibliothèques algorithmiques abordent certaines tâches d'apprentissage automatique, qu'elles soient basiques ou complexes. Nous présentons ici deux packages. La bibliothèque Scikit-learn contient des outils pour la modélisation statistique, notamment la régression, la classification, le clustering, etc. Cette bibliothèque est construite sur NumPy, SciPy et Matplotlib. Statsmodels est également un module Python qui permet aux utilisateurs d'explorer des données, d' estimer des modèles statistiques et d'effectuer des tests statistiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1c8e7-cc38-4b8a-bc9a-71dcbff51a88",
   "metadata": {},
   "source": [
    "# Importer et exporter des données en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31eab5-2d27-4b5e-8a6d-6b3621a4280e",
   "metadata": {},
   "source": [
    "nous verrons comment lire des données à l'aide du package Pandas de Python. Une fois que nous avons nos données en Python, nous pouvons effectuer toutes les procédures d'analyse de données ultérieures dont nous avons besoin. L'acquisition de données est un processus de chargement et de lecture de données dans un bloc-notes à partir de diverses sources. Pour lire des données à l'aide du package Pandas de Python, deux facteurs importants doivent être pris en compte : le format et le chemin du fichier. Le format est la façon dont les données sont codées. Nous pouvons généralement identifier les différents schémas de codage en regardant la fin du nom du fichier. Certains encodages courants sont CSV, JSON, XLSX, HDF, etc. Le chemin nous indique où les données sont stockées. Il est généralement stocké soit sur l'ordinateur que nous utilisons, soit en ligne sur Internet. Dans notre cas, nous avons trouvé un ensemble de données de voitures d'occasion obtenu à partir de l' adresse Web indiquée sur la diapositive. Lorsque Jerry a saisi l'adresse Web dans son navigateur Web, il a vu quelque chose comme ceci. Chaque ligne correspond à un point de données. Un grand nombre de propriétés sont associées à chaque point de données. Comme les propriétés sont séparées les unes des autres par des virgules, nous pouvons deviner que le format des données est CSV, qui signifie valeurs séparées par des virgules. À ce stade, ce ne sont que des chiffres qui ne signifient pas grand-chose pour les humains. Mais une fois que nous aurons lu ces données, nous pourrons essayer d'en tirer un meilleur sens. Dans Pandas, la méthode read_csv permet de lire des fichiers dont les colonnes sont séparées par des virgules dans un bloc de données Pandas. La lecture des données dans Pandas peut être effectuée rapidement en trois lignes. Importez d'abord des Pandas, puis définissez une variable avec un chemin de fichier. Ensuite, utilisez la méthode read_csv pour importer les données. Cependant, read_csv suppose que les données contiennent un en-tête. Nos données sur les voitures d'occasion ne comportent aucun en-tête de colonne. Nous devons donc spécifier read_csv pour ne pas attribuer d'en-têtes en définissant l'en-tête sur none. Après avoir lu l'ensemble de données, il est conseillé d'examiner le bloc de données pour avoir une meilleure intuition et pour s'assurer que tout s'est passé comme prévu. Étant donné que l'impression de l'ensemble de données peut prendre trop de temps et de ressources, pour gagner du temps, nous pouvons simplement utiliser dataframe.head pour afficher les n premières lignes du bloc de données. De même, dataframe.tail affiche les n dernières lignes du bloc de données. Ici, nous avons imprimé les cinq premières lignes de données. Il semble que l'ensemble de données ait été lu avec succès. Nous pouvons voir que Pandas définit automatiquement l'en-tête de colonne sous forme de liste d'entiers, car nous définissons header= none, lorsque nous lisons les données. Il est difficile de travailler avec le bloc de données sans avoir de noms de colonnes significatifs. Cependant, nous pouvons attribuer des noms de colonnes dans Pandas. Dans le cas présent, il s'est avéré que nous avons les noms des colonnes dans un fichier séparé en ligne. Nous avons d'abord mis les noms des colonnes dans une liste appelée en-têtes. Ensuite, nous définissons df.columns=headers pour remplacer les en-têtes entiers par défaut par la liste. Si nous utilisons la méthode de la tête présentée dans la dernière diapositive pour vérifier l'ensemble de données, nous voyons les bons en-têtes insérés en haut de chaque colonne. À un moment donné, après avoir effectué des opérations sur votre bloc de données, vous souhaiterez peut-être exporter votre bloc de données Pandas vers un nouveau fichier CSV. Vous pouvez le faire en utilisant la méthode to_csv. Pour ce faire, spécifiez le chemin du fichier, qui inclut le nom du fichier dans lequel vous souhaitez écrire. Par exemple, si vous souhaitez enregistrer le bloc de données df sous le nom de automobile.csv sur votre propre ordinateur, vous pouvez utiliser la syntaxe df.2_csv. Pour ce cours, nous ne lirons et enregistrerons que des fichiers CSV. Cependant, Pandas prend également en charge l'importation et l' exportation de la plupart des types de fichiers de données avec différents formats d'ensembles de données. La syntaxe du code pour lire et enregistrer d'autres formats de données est très similaire à celle du code de lecture ou d'enregistrement d'un fichier CSV. Chaque colonne indique une méthode différente pour lire et enregistrer des fichiers dans un format différent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331cb05-423c-4569-a31e-e445f6a88e7e",
   "metadata": {},
   "source": [
    " # Commencer à analyser des données en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c54ab1-155e-41d1-a0e9-4d265115c0cd",
   "metadata": {},
   "source": [
    "nous présentons quelques méthodes Pandas simples que tous les data scientists et analystes devraient connaître lorsqu'ils utilisent Python, Pandas et les données. À ce stade, nous supposons que les données ont été chargées. Il est temps pour nous d'explorer l'ensemble de données. Pandas dispose de plusieurs méthodes intégrées qui peuvent être utilisées pour comprendre le type de données des caractéristiques ou pour examiner la distribution des données dans l'ensemble de données. L'utilisation de ces méthodes donne une vue d'ensemble de l'ensemble de données et met également en évidence les problèmes potentiels tels que le mauvais type de données ou les caractéristiques qui devront peut-être être résolus ultérieurement. Les données sont de différents types. Les principaux types stockés dans les objets Pandas sont object, float, int et datetime. Les noms des types de données sont quelque peu différents de ceux du Python natif. Ce tableau montre les différences et les similitudes entre les deux. Certains sont très similaires, comme les types de données numériques int et float. Le type d'objet pandas fonctionne de la même manière que la chaîne en Python, à l'exception du changement de nom. Bien que le type Pandas date et heure soit un type très utile pour gérer les données de séries chronologiques, il existe deux raisons de vérifier les types de données dans un ensemble de données. Pandas attribue automatiquement des types en fonction de l'encodage détecté dans la table de données d'origine. Pour un certain nombre de raisons, cette attribution peut être incorrecte. Par exemple, il serait gênant que la colonne des prix des voitures, qui devrait contenir des chiffres continus, soit affectée au type de données de l'objet. Il serait plus naturel qu'il soit du type flottant. Jerry devra peut-être modifier manuellement le type de données en flottant. La deuxième raison est qu'il permet à un data scientist expérimenté de voir quelles fonctions Python peuvent être appliquées à une colonne spécifique. Par exemple, certaines fonctions mathématiques ne peuvent être appliquées qu'à des données numériques. Si ces fonctions sont appliquées à des données non numériques, une erreur peut se produire. Lorsque la méthode de type de données est appliquée à l'ensemble de données. Le type de données de chaque colonne est renvoyé dans une série. L'intuition d'un bon data scientist nous dit que la plupart des types de données ont du sens. Ils fabriquent des voitures, par exemple des noms. Ces informations doivent être de type objet. Le dernier de la liste poserait problème. L'alésage étant une dimension d'un moteur, il faut s'attendre à ce qu'un type de données numérique soit utilisé. Au lieu de cela, le type d'objet est utilisé. Dans les sections suivantes, Jerry devra corriger ces types de discordances. Nous aimerions maintenant consulter le résumé statistique de chaque colonne pour en savoir plus sur la distribution des données dans chaque colonne. Les métriques statistiques peuvent indiquer au data scientist si des problèmes mathématiques peuvent exister, tels que des valeurs extrêmes et des écarts importants, les data scientists devront peut-être résoudre ces problèmes ultérieurement. Pour obtenir les statistiques rapides, nous utilisons la méthode décrite. Elle renvoie le nombre de termes de la colonne sous forme de nombre, la valeur moyenne de colonne sous forme de moyenne, l' écart type de colonne sous forme de STD et les valeurs minimales maximales, ainsi que la limite de chacun des quartiles. Par défaut, les fonctions décrites dans le bloc de données ignorent les lignes et les colonnes qui ne contiennent pas de chiffres. Il est également possible de faire fonctionner la méthode décrite pour les colonnes de type d'objet. Pour permettre un résumé de toutes les colonnes, nous pourrions ajouter un argument, y compris des valeurs égales, le tout à l'intérieur du crochet de fonction décrit. Le résultat affiche maintenant le résumé des 26 colonnes, y compris les attributs du type d'objet. Nous voyons que pour les colonnes de type d'objet, un ensemble de statistiques différent est évalué comme étant unique, supérieur et fréquence. Unique est le nombre d'objets distincts dans la colonne, en haut correspond à l'objet qui apparaît le plus fréquemment et fréquent est le nombre de fois que l'objet supérieur apparaît dans la colonne. Certaines valeurs du tableau sont indiquées ici sous forme de NaN, ce qui ne signifie pas un nombre. Cela est dû au fait que cette métrique statistique particulière ne peut pas être calculée pour ce type de données de colonne spécifique. Une autre méthode que vous pouvez utiliser pour vérifier votre ensemble de données est la fonction dataframe.info. Cette fonction fournit un résumé concis de la trame de données. Cette méthode imprime des informations sur un bloc de données, y compris le type d'index D et les colonnes, les valeurs non nulles et l'utilisation de la mémoire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662b42e-1cbe-44a5-bedb-2bb3f9d923ba",
   "metadata": {},
   "source": [
    "# Accéder aux bases de données avec Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7baa9-2f12-4f09-b24b-5d3db1d6769d",
   "metadata": {},
   "source": [
    "Bonjour, dans cette vidéo, vous allez apprendre à accéder à des bases de données en Python. Les bases de données sont de puissants outils pour les data scientists. Après avoir terminé ce module, vous serez en mesure d'expliquer les concepts de base liés à l'utilisation de Python pour la connexion aux bases de données. C'est ainsi qu'un utilisateur typique accède aux bases de données à l'aide de code Python écrit sur un Jupiter Notebook, un éditeur Web. Il existe un mécanisme par lequel le programme Python communique avec le SGBD. Le code Python se connecte à la base de données à l'aide d'appels d'API. Nous expliquerons les bases des API SQL et des API de base de données Python. Une interface de programmation d'applications est un ensemble de fonctions que vous pouvez appeler pour accéder à un type de service. Une API SQL consiste en des appels de fonctions de bibliothèque en tant qu' API d'interface de programmation d'applications permettant au SGBD de transmettre des instructions SQL au SGBD, un programme d'application appelle des fonctions dans l'API et appelle d'autres fonctions pour récupérer les résultats des requêtes et les informations d'état du SGBD. Le fonctionnement de base d' une API SQL classique est illustré dans la figure. Le programme d'application commence son accès à la base de données par un ou plusieurs appels d'API qui connectent le programme au SGBD. Pour envoyer une instruction SQL au SGBD, le programme construit l'instruction sous forme de chaîne de texte dans une mémoire tampon, puis effectue un appel d'API pour transmettre le contenu de la mémoire tampon au SGBD. Le programme d'application effectue des appels d'API pour vérifier l'état de sa demande DBMS et pour gérer les erreurs. Le programme d'application met fin à son accès à la base de données par un appel d'API qui le déconnecte de la base de données. L'API DB est l' API standard de Python pour accéder aux bases de données relationnelles. Il s'agit d'une norme qui vous permet d' écrire un seul programme qui fonctionne avec plusieurs types de bases de données relationnelles au lieu d'écrire un programme distinct pour chacune d'entre elles. Si vous apprenez les fonctions de l'API de base de données, vous pouvez appliquer ces connaissances pour utiliser n'importe quelle base de données avec Python. Les deux principaux concepts de l'API Python DB sont les objets de connexion et les objets de requête. Vous utilisez des objets de connexion pour vous connecter à une base de données et gérer vos transactions. Les objets Cursor sont utilisés pour exécuter des requêtes. Vous ouvrez un objet curseur, puis vous exécutez des requêtes. Le curseur fonctionne de la même manière qu'un curseur dans un système de traitement de texte, dans lequel vous faites défiler le jeu de résultats vers le bas pour transférer vos données dans l'application. Les curseurs sont utilisés pour parcourir les résultats d'une base de données. Voici les méthodes utilisées avec les objets de connexion. La méthode cursor renvoie un nouvel objet curseur à l'aide de la connexion. La méthode commit est utilisée pour valider toute transaction en attente dans la base de données. La méthode de restauration permet à la base de données de revenir au début de toute transaction en attente. La méthode fermée est utilisée pour fermer une connexion à une base de données. Découvrons une application Python qui utilise l'API de base de données pour interroger une base de données. Tout d' abord, vous importez votre module de base de données à l'aide de l'API de connexion de ce module. Pour ouvrir une connexion à la base de données, vous utilisez la fonction de connexion et transmettez les paramètres, à savoir le nom de la base de données, le nom d'utilisateur et le mot de passe. La fonction connect renvoie un objet de connexion. Ensuite, vous créez un objet curseur sur l'objet de connexion. Le curseur est utilisé pour exécuter des requêtes et récupérer les résultats. Après avoir exécuté les requêtes à l'aide du curseur, nous utilisons également le curseur pour récupérer les résultats de la requête. Enfin, lorsque le système a fini d'exécuter les requêtes, il libère toutes les ressources en fermant la connexion. N'oubliez pas qu'il est toujours important de fermer les connexions pour éviter que les connexions inutilisées n'utilisent des ressources. Merci d'avoir regardé cette vidéo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aff4b7f-260e-4db7-892f-082481d5151e",
   "metadata": {},
   "source": [
    "# Résumé de la leçonStatut :\n",
    "\n",
    "\n",
    "Nous vous félicitons ! Vous avez terminé cette leçon. À ce stade du cours, vous savez ce qui suit :\n",
    "\n",
    "Chaque ligne d'un ensemble de données est une ligne et les virgules séparent les valeurs.\n",
    "\n",
    "Pour comprendre les données, vous devez analyser les attributs de chaque colonne de données.\n",
    "\n",
    "- Les bibliothèques Python sont des collections de fonctions et de méthodes qui facilitent diverses fonctionnalités sans avoir à écrire du code à partir de zéro. Elles sont classées dans les catégories suivantes : calcul scientifique, visualisation des données et algorithmes d'apprentissage automatique.\n",
    "\n",
    "- De nombreuses bibliothèques de science des données sont interconnectées ; par exemple, Scikit-learn est construit au-dessus de NumPy, SciPy et Matplotlib.\n",
    "\n",
    "- Le format des données et le chemin d'accès au fichier sont deux facteurs clés pour la lecture des données avec Pandas.\n",
    "\n",
    "-  La méthode read_CSV de Pandas permet de lire des fichiers au format CSV dans un DataFrame Pandas.\n",
    "\n",
    "- Pandas possède des types de données uniques tels que object, float, Int et datetime.\n",
    "\n",
    "- Utilisez la méthode dtype pour vérifier le type de données de chaque colonne ; les types de données mal classés peuvent nécessiter une correction manuelle.\n",
    "\n",
    "- Connaître les types de données corrects permet d'appliquer les fonctions Python appropriées à des colonnes spécifiques.\n",
    "\n",
    "- L'utilisation de Statistical Summary avec describe() permet d'obtenir le nombre, la moyenne, l'écart type, les valeurs min, max et les intervalles de quartiles pour les colonnes numériques.\n",
    "\n",
    "- Vous pouvez également utiliser include='all' comme argument pour obtenir des résumés pour les colonnes de type objet.\n",
    "\n",
    "- Le résumé statistique permet d'identifier les problèmes potentiels tels que les valeurs aberrantes nécessitant une attention particulière.\n",
    "\n",
    "- L'utilisation de la méthode info() permet d'obtenir une vue d'ensemble des 30 premières et 30 dernières lignes du DataFrame, utile pour une inspection visuelle rapide.\n",
    "\n",
    "- Certaines mesures statistiques peuvent renvoyer \"NaN\", indiquant des valeurs manquantes, et le programme ne peut pas calculer de statistiques pour ce type de données spécifique.\n",
    "\n",
    "- Python peut se connecter aux bases de données par le biais d'un code spécialisé, souvent écrit dans les carnets Jupyter.\n",
    "\n",
    "- Les interfaces de programmation d'applications (API) SQL et les API DB Python (les plus souvent utilisées) facilitent l'interaction entre Python et le SGBD.\n",
    "\n",
    "- LesAPI SQL se connectent au SGBD avec un ou plusieurs appels API, construisent des instructions SQL sous forme de chaîne de texte et utilisent les appels API pour envoyer des instructions SQL au SGBD et récupérer les résultats et les statuts.\n",
    "\n",
    "- DB-API, Standard de Python pour l'interaction avec les bases de données relationnelles, utilise des objets de connexion pour établir et gérer les connexions aux bases de données et des objets de curseur pour exécuter des requêtes et faire défiler les résultats.\n",
    "\n",
    "- Les méthodes des objets de connexion comprennent les commandes cursor(), commit(), rollback() et close().\n",
    "\n",
    "- Vous pouvez importer le module de base de données, utiliser l'API Connect pour ouvrir une connexion, puis créer un objet curseur pour exécuter des requêtes et extraire des résultats.\n",
    "\n",
    "N'oubliez pas de fermer la connexion à la base de données pour libérer des ressources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d0c4e-8a26-4cd0-8ec6-6e965b8cf0e3",
   "metadata": {},
   "source": [
    "# Prétraitement des données en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b49d3-e9de-4291-b3ff-0f8754d2a3eb",
   "metadata": {},
   "source": [
    "nous allons passer en revue certaines techniques de prétraitement des données. Si ce terme ne vous est pas familier, le prétraitement des données est une étape nécessaire de l'analyse des données. Il s'agit du processus de conversion ou de mappage des données d' une forme brute dans un autre format afin de les préparer à une analyse plus approfondie. Le prétraitement des données est souvent appelé « nettoyage des données » ou « data wrangling », et il existe probablement d'autres termes. Voici les sujets que nous aborderons dans ce module. Tout d'abord, nous allons vous montrer comment identifier et gérer les valeurs manquantes. Une condition de valeur manquante se produit chaque fois qu'une entrée de données est laissée vide. Nous aborderons ensuite les formats de données. Les données provenant de différentes sources peuvent être présentées dans différents formats, dans différentes unités ou selon différentes conventions. Nous allons introduire quelques méthodes dans Python Pandas qui permettent de standardiser les valeurs dans le même format , unité ou convention. Ensuite, nous aborderons la normalisation des données. Les différentes colonnes de données numériques peuvent avoir des plages très différentes, et la comparaison directe n'est souvent pas significative. La normalisation est un moyen de placer toutes les données dans une plage similaire pour une comparaison plus utile. Plus précisément, nous nous concentrerons sur les techniques de centrage et de mise à l'échelle, puis nous introduirons le regroupement des données. Le binning crée de plus grandes catégories à partir d'un ensemble de valeurs numériques. Il est particulièrement utile pour comparer des groupes de données. Enfin, nous parlerons des variables catégorielles et vous montrerons comment convertir des valeurs catégorielles en variables numériques pour faciliter la modélisation statistique. En Python, nous effectuons généralement des opérations le long des colonnes. Chaque ligne de la colonne représente un échantillon, c'est-à-dire une voiture d'occasion différente dans la base de données. Vous pouvez accéder à une colonne en spécifiant le nom de la colonne. Par exemple, vous pouvez accéder à la symbolique et au style corporel. Chacune de ces colonnes est une série Panda. Il existe de nombreuses manières de manipuler les blocs de données en Python. Par exemple, vous pouvez ajouter une valeur à chaque entrée d'une colonne. Pour en ajouter un à chaque entrée symbolique, utilisez cette commande. Cela modifie chaque valeur de la colonne du bloc de données en ajoutant une valeur à la valeur actuelle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6185709-9c66-4660-9ddc-7abdfb7f0cc2",
   "metadata": {},
   "source": [
    "# Traiter les valeurs manquantes en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31ec0c-13b1-4d7f-b9de-2ca7320634d1",
   "metadata": {},
   "source": [
    "Dans cette vidéo, nous allons présenter le problème omniprésent des valeurs manquantes ainsi que les stratégies à adopter lorsque vous rencontrez des valeurs manquantes dans vos données. Lorsqu'aucune valeur de données n'est stockée pour une caractéristique pour une observation particulière, nous disons que cette caractéristique a une valeur manquante. Habituellement, la valeur manquante dans l'ensemble de données apparaît sous forme de point d'interrogation, de N/A, de zéro ou simplement d'une cellule vide. Dans l'exemple ci-dessous, la fonction de pertes normalisées a une valeur manquante qui est représentée par NaN. Mais comment gérer les données manquantes ? Il existe de nombreuses façons de traiter les valeurs manquantes, indépendamment de Python, R ou de tout autre outil que vous utilisez. Bien entendu, chaque situation est différente et doit être jugée différemment. Cependant, ce sont les options typiques que vous pouvez envisager. La première consiste à vérifier si la personne ou le groupe qui a collecté les données peut revenir en arrière et trouver la valeur réelle. Une autre possibilité consiste simplement à supprimer les données où se trouve cette valeur manquante. Lorsque vous supprimez des données, vous pouvez soit supprimer la variable entière, soit uniquement l'entrée de données contenant la valeur manquante. Si vous n'avez pas beaucoup d'observations comportant des données manquantes, il est généralement préférable de supprimer l'entrée en question. Si vous supprimez des données, vous devez essayer de faire quelque chose qui a le moins d'impact possible. Il est préférable de remplacer les données car aucune donnée n'est gaspillée. Cependant, il est moins précis car nous devons remplacer les données manquantes par une estimation de ce que devraient être les données. Une technique de remplacement standard consiste à remplacer les valeurs manquantes par la valeur moyenne de la variable entière. Par exemple, supposons que certaines entrées présentent des valeurs manquantes pour la colonne des pertes normalisées et que la moyenne de la colonne pour les entrées contenant des données soit de 4 500. Bien qu'il n'existe aucun moyen pour nous d'obtenir une estimation précise des valeurs manquantes dans la colonne des pertes normalisées, vous pouvez approximer leurs valeurs en utilisant la valeur moyenne de la colonne, 4 500. Mais que se passe-t-il si les valeurs ne peuvent pas être moyennées comme pour les variables catégorielles ? Pour une variable telle que le type de carburant, il n'existe pas de type de carburant moyen puisque les valeurs des variables ne sont pas des nombres. Dans ce cas, une possibilité est d'essayer d'utiliser le mode le plus courant, comme l'essence. Enfin, nous pouvons parfois trouver un autre moyen de deviner les données manquantes. Cela est généralement dû au fait que le collecteur de données sait quelque chose de plus sur les données manquantes. Par exemple, il sait peut-être que les valeurs manquantes correspondent généralement à de vieilles voitures et que les pertes normalisées des voitures anciennes sont nettement plus élevées que celles d'un véhicule moyen. Et bien sûr, dans certains cas, vous voudrez peut-être simplement laisser les données manquantes comme données manquantes. Pour une raison ou une autre, il peut être utile de garder cette observation même si certaines fonctionnalités sont absentes. Voyons maintenant comment supprimer les valeurs manquantes ou remplacer les valeurs manquantes en Python. Pour supprimer les données contenant des valeurs manquantes, la bibliothèque Pandas dispose d'une méthode intégrée appelée dropna. Essentiellement, avec la méthode dropna, vous pouvez choisir de supprimer les lignes ou les colonnes contenant des valeurs manquantes, comme NaN. Vous devrez donc spécifier axis = 0 pour supprimer les lignes, ou axis = 1 pour supprimer les colonnes contenant les valeurs manquantes. Dans cet exemple, il manque une valeur dans la colonne des prix. Comme le prix des voitures d'occasion est celui que nous essayons de prévoir dans notre prochaine analyse, nous devrons supprimer les voitures, les rangées dont le prix n'est pas indiqué. Cela peut simplement être fait en une seule ligne de code à l'aide de dataframe.dropna. Si l'argument in place est défini sur True, la modification peut être effectuée directement sur l'ensemble de données, inplace = True écrit simplement le résultat dans le bloc de données. Cela équivaut à cette ligne de code. N'oubliez pas que cette ligne de code ne modifie pas le bloc de données, mais c'est un bon moyen de vous assurer que vous effectuez la bonne opération. Pour modifier le bloc de données, vous devez définir le paramètre inplace = True. Vous devez toujours consulter la documentation si vous ne connaissez pas une fonction ou une méthode. La page Web des Pandas contient de nombreuses ressources utiles. Pour remplacer les valeurs manquantes telles que les NaN par des valeurs réelles, la bibliothèque Pandas dispose d'une méthode intégrée appelée Replace, qui peut être utilisée pour remplir les valeurs manquantes avec les valeurs nouvellement calculées. Par exemple, supposons que nous voulions remplacer les valeurs manquantes de la variable pertes normalisées par la valeur moyenne de la variable. Par conséquent, la valeur manquante doit être remplacée par la moyenne des entrées de cette colonne. En Python, nous calculons d'abord la moyenne de la colonne. Ensuite, nous utilisons la méthode Replace pour spécifier la valeur que nous voulons remplacer comme premier paramètre, dans ce cas NaN. Le deuxième paramètre est la valeur par laquelle nous aimerions le remplacer, c'est-à-dire la moyenne dans cet exemple. Il s'agit d'une méthode assez simplifiée pour remplacer les valeurs manquantes. Il existe bien entendu d'autres techniques, telles que le remplacement des valeurs manquantes pour la moyenne du groupe plutôt que pour l'ensemble de données complet.\n",
    "Lisez la vidéo à partir de :5:34 et suivez la transcription5:34\n",
    "Nous avons donc utilisé deux méthodes en Python pour traiter les données manquantes. Nous avons appris à supprimer les lignes ou les colonnes problématiques contenant des valeurs manquantes, puis nous avons appris à remplacer les valeurs manquantes par d'autres valeurs. Mais n'oubliez pas les autres moyens de traiter les données manquantes. Vous pouvez toujours rechercher un ensemble de données ou une source de meilleure qualité ou, dans certains cas, vous pouvez laisser les données manquantes en tant que données manquantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e74dcd-80eb-4dc2-9e0f-ddb279b3e283",
   "metadata": {},
   "source": [
    "# Formatage des données en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b910a7-b5ff-446f-b544-77c208653755",
   "metadata": {},
   "source": [
    "nous aborderons le problème des données présentant des formats, des unités et des conventions différents, ainsi que les méthodes Pandas qui nous aident à résoudre ces problèmes. Les données sont généralement collectées à différents endroits, par différentes personnes , et peuvent être stockées dans différents formats. Le formatage des données consiste à intégrer les données dans une norme d' expression commune qui permet aux utilisateurs de faire des comparaisons significatives. Dans le cadre du nettoyage des ensembles de données, le formatage des données garantit la cohérence et la facilité de compréhension des données. Par exemple, les gens peuvent utiliser différentes expressions pour représenter la ville de New York, telles que N.Y., Ny , NY et New York. Parfois, ces données impures sont une bonne chose à voir. Par exemple, si vous examinez les différentes manières dont les gens ont tendance à écrire New York, alors ce sont exactement les données que vous recherchez. Ou si vous cherchez des moyens de détecter les fraudes, peut-être qu'écrire à New York est plus susceptible de prédire une anomalie que si quelqu'un écrivait New York dans son intégralité. Mais peut-être que le plus souvent, nous voulons simplement les traiter tous comme la même entité ou le même format afin de faciliter l'analyse statistique à l'avenir. En référence à notre ensemble de données sur les voitures d'occasion, le jeu de données contient une fonctionnalité nommée miles urbains par gallon, qui fait référence à la consommation de carburant d'une voiture en miles par gallon. Cependant, vous vivez peut-être dans un pays qui utilise des unités métriques. Vous souhaiterez donc convertir ces valeurs en litres par 100 kilomètres, la version métrique. Pour transformer les miles par gallon en litres par 100 kilomètres, nous devons diviser 235 par chaque valeur dans la colonne des miles par gallon de la ville. En Python, cela peut facilement être fait en une seule ligne de code. Vous prenez la colonne et vous la définissez comme égale à 235 divisée par la colonne entière. Dans la deuxième ligne de code, renommez le nom de colonne de miles urbains par gallon en litres-villes par 100 kilomètres à l' aide de la méthode de renommage du bloc de données. Pour un certain nombre de raisons, notamment lorsque vous avez importé un ensemble de données dans Python, le type de données peut être mal établi. Par exemple, nous remarquons ici que le type de données attribué à la fonction de prix est objet alors que le type de données attendu doit en réalité être un entier ou un nombre flottant. Pour une analyse ultérieure, il est important d'explorer le type de données de l'entité et de les convertir en types de données appropriés. Sinon, les modèles développés ultérieurement risquent de se comporter de manière étrange et des données totalement valides pourraient finir par être traitées comme des données manquantes. Il existe de nombreux types de données dans Pandas. Les objets peuvent être des lettres ou des mots. Les int64 sont des entiers et les flottants sont des nombres réels. Il y en a beaucoup d'autres dont nous ne parlerons pas. Pour identifier le type de données d'une fonctionnalité en Python, nous pouvons utiliser la méthode dataframe.dtypes et vérifier le type de données de chaque variable dans un bloc de données. Dans le cas de types de données incorrects, la méthode dataframe.astype peut être utilisée pour convertir un type de données d'un format à un autre. Par exemple, en utilisant astype (« int ») pour la colonne de prix, vous pouvez convertir la colonne d'objet en une variable de type entier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e3a86f-5471-4b93-a388-ac5bc282162c",
   "metadata": {},
   "source": [
    " # Normalisation des données en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be349db-b070-4b3a-b133-b0f3757a7041",
   "metadata": {},
   "source": [
    "nous allons parler de la normalisation des données, une technique importante à comprendre dans le cadre du prétraitement des données. Lorsque nous examinons l'ensemble de données sur les voitures d'occasion, nous remarquons que la longueur des caractéristiques varie de 150 à 250, tandis que la largeur et la hauteur des caractéristiques vont de 50 à 100. Nous souhaiterons peut-être normaliser ces variables afin que la plage des valeurs soit cohérente. Cette normalisation peut faciliter certaines analyses statistiques par la suite. En rendant les plages cohérentes entre les variables. La normalisation permet une comparaison plus juste entre les différentes fonctionnalités, en s'assurant qu'elles ont le même impact. C'est également important pour des raisons informatiques. Voici un autre exemple qui vous aidera à comprendre pourquoi la normalisation est importante. Prenons l'exemple d'un ensemble de données contenant deux caractéristiques : l'âge et le revenu, où l'âge varie de 0 à 100 ans, tandis que le revenu varie de 0 à 20 000 ans et plus. Le revenu est environ 1 000 fois supérieur à l'âge et varie de 20 000 à 500 000. Ces deux fonctionnalités se situent donc dans des gammes très différentes. Lorsque nous effectuons une analyse plus approfondie, comme une régression linéaire, par exemple, l'attribut « revenu » influencera intrinsèquement davantage le résultat en raison de sa valeur plus élevée. Mais cela ne signifie pas nécessairement qu'il est plus important en tant que prédicteur. La nature des données biaise donc le modèle de régression linéaire de manière à ce qu'il pondère le revenu de manière plus importante que l'âge. Pour éviter cela, nous pouvons normaliser ces deux variables en valeurs comprises entre 0 et 1. Comparez les deux tableaux sur la droite. Après normalisation, les deux variables ont désormais une influence similaire sur les modèles que nous développerons ultérieurement. Il existe plusieurs méthodes pour normaliser les données. Je vais simplement décrire trois techniques. La première méthode, appelée mise à l'échelle simple des entités, divise simplement chaque valeur par la valeur maximale de cette entité. Les nouvelles valeurs sont donc comprises entre 0 et 1. La deuxième méthode, appelée min max, prend chaque valeur x underscore old, la soustrait de la valeur minimale de cette caractéristique, puis la divise par la plage de cette caractéristique. Là encore, les nouvelles valeurs obtenues sont comprises entre 0 et 1. La troisième méthode est appelée Z-score, ou score standard. Dans cette formule, pour chaque valeur, vous soustrayez le mu, qui est la moyenne de la caractéristique, puis vous le divisez par l'écart type sigma. Les valeurs obtenues oscillent autour de zéro et se situent généralement entre moins trois et plus trois, mais elles peuvent être supérieures ou inférieures. En suivant notre exemple précédent, nous pouvons appliquer la méthode de normalisation à la caractéristique de longueur. Tout d'abord, nous utilisons la méthode simple de mise à l'échelle des entités, qui consiste à la diviser par la valeur maximale de l'entité. En utilisant la méthode Pandas max (), cela peut être fait en une seule ligne de code. Voici la méthode min.max pour la fonction de longueur. Nous soustrayons chaque valeur par le minimum de cette colonne, puis nous la divisons par la plage de cette colonne, le maximum moins le minimum. Enfin, nous appliquons la méthode du score Z sur la caractéristique de longueur pour normaliser les valeurs. Ici, nous appliquons la méthode de la moyenne et de la méthode STD à la caractéristique de longueur. La méthode moyenne renvoie la valeur moyenne de l'entité dans l'ensemble de données et la méthode STD renvoie l'écart type des entités dans l'ensemble de données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5dee2d-8304-4831-9573-ba5b5143597c",
   "metadata": {},
   "source": [
    "# Binning en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80bb659-378c-4b0a-91ee-1263bd874ea0",
   "metadata": {},
   "source": [
    "nous allons parler du binning en tant que méthode de prétraitement des données. Le binning consiste à regrouper des valeurs dans des groupes. Par exemple, vous pouvez regrouper l'âge en 0-5 ans, 6-10 ans, 11-15 ans, etc. Parfois, le binning peut améliorer la précision des modèles prédictifs. En outre, nous utilisons parfois le regroupement des données pour regrouper un ensemble de valeurs numériques dans un plus petit nombre de groupes afin de mieux comprendre la distribution des données. Par exemple, prix, voici une plage d'attributs comprise entre 5 000 et 45 500. À l'aide du binning, nous classons le prix en trois catégories : prix bas, prix moyen et prix élevé. Dans le jeu de données actuel sur les voitures, le prix est une variable numérique comprise entre 5 188 et 45 400, il comporte 201 valeurs uniques. Nous pouvons les classer en trois catégories : voitures à prix bas, moyen et élevé. En Python, nous pouvons facilement implémenter le binning. Nous aimerions trois groupes de largeur égale. Nous avons donc besoin de quatre nombres séparés par quatre nombres à égale distance les uns des autres. Tout d'abord, nous utilisons la fonction NumPy linspace pour renvoyer les groupes de tableaux contenant quatre nombres espacés de manière égale sur l'intervalle spécifié du prix. Nous créons une liste de noms de soulignement contenant les différents noms de casiers. Nous utilisons la fonction Pandas cut pour segmenter et trier les valeurs des données dans des groupes. Vous pouvez ensuite utiliser des histogrammes pour visualiser la distribution des données une fois qu'elles ont été divisées en groupes. Il s'agit de l'histogramme que nous avons tracé en fonction de l'enchère que nous avons appliquée dans la rubrique prix. L'intrigue montre clairement que la plupart des voitures ont un prix bas et que très peu de voitures ont un prix élevé. [MUSIQUE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bef0eb-11e6-4e12-8b38-dd80e5e41548",
   "metadata": {},
   "source": [
    "# Transformer des variables catégorielles en variables quantitatives en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6438022-d64f-4679-a6b3-125ead309734",
   "metadata": {},
   "source": [
    "nous verrons comment transformer des variables catégorielles en variables quantitatives en Python. La plupart des modèles statistiques ne peuvent pas prendre d' objets ou de chaînes en entrée, et pour l'apprentissage des modèles, ils ne prennent que les nombres comme entrées. Dans l'ensemble de données de la voiture, la fonction de type de carburant en tant que variable catégorielle comporte deux valeurs : essence ou diesel, qui sont sous forme de chaîne. Pour une analyse plus approfondie, Jerry doit convertir ces variables dans un format numérique. Nous encodons les valeurs en ajoutant de nouvelles fonctionnalités correspondant à chaque élément unique de la fonctionnalité d'origine que nous aimerions encoder. Dans le cas où le carburant caractéristique possède deux valeurs uniques, l' essence et le diesel, nous créons deux nouvelles fonctionnalités, l'essence et le diesel. Lorsqu'une valeur apparaît dans la fonction d'origine, nous définissons la valeur correspondante sur un dans la nouvelle fonction, les autres fonctionnalités sont mises à zéro. Dans l'exemple du carburant pour le véhicule B, la valeur du carburant est le diesel. Par conséquent, nous avons réglé la caractéristique diesel égale à un et la fonction essence à zéro. De même pour la voiture D, la valeur du carburant est l'essence. Par conséquent, nous avons défini le gaz caractéristique égal à un et le diesel égal à zéro. Cette technique est souvent appelée « encodage à chaud ». Chez les pandas, nous pouvons utiliser la méthode get_dummies pour convertir des variables catégorielles en variables factices. En Python, il est simple de transformer des variables catégorielles en variables fictives.\n",
    "Lisez la vidéo à partir de :1:51 et suivez la transcription1:51\n",
    "En suivant l'exemple, la méthode pd.get_dummies obtient la colonne de type de carburant et crée le bloc de données dummy_variable_one. La méthode get_dummies génère automatiquement une liste de nombres, chacun correspondant à une catégorie particulière de la variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba16886-789f-455a-9e70-66d53a45b6ba",
   "metadata": {},
   "source": [
    "# Résumé de la leçonStatut : Traduit automatiquement de Anglais\n",
    "Traduit automatiquement de Anglais\n",
    "Nous vous félicitons ! Vous avez terminé cette leçon. À ce stade du cours, vous savez :\n",
    "\n",
    "- Le formatage des données est essentiel pour rendre les données provenant de différentes sources cohérentes et comparables.\n",
    "\n",
    "- Maîtriser les techniques de conversion des unités de mesure en Python, comme la transformation des \"miles urbains par gallon\" en \"litres urbains par 100 kilomètres\" pour faciliter la comparaison et l'analyse.\n",
    "\n",
    "- Acquérir des compétences pour identifier et corriger les types de données en Python, afin de s'assurer que les données sont représentées avec précision pour les analyses statistiques ultérieures.\n",
    "\n",
    "- La normalisation des données permet de rendre les variables comparables et d'éliminer les biais inhérents aux modèles statistiques.\n",
    "\n",
    "- Vous pouvez appliquer les techniques Feature Scaling, Min-Max et Z-Score pour normaliser les données et appliquer chaque technique en Python à l'aide des méthodes de pandas.\n",
    "\n",
    "- Le binning est une méthode de prétraitement des données qui permet d'améliorer la précision des modèles et la visualisation des données.\n",
    "\n",
    "- Exécutez les techniques de regroupement en Python à l'aide des méthodes numpy \"linspace\" et pandas \"cut\", en particulier pour les variables numériques comme le \"prix\"\n",
    "\n",
    "- Utilisez les histogrammes pour visualiser la distribution des données binées et obtenir des informations sur la distribution des caractéristiques.\n",
    "\n",
    "- Les modèles statistiques nécessitent généralement des entrées numériques, ce qui rend nécessaire la conversion de variables catégorielles telles que le \"type de carburant\" en formats numériques.\n",
    "\n",
    "- Vous pouvez mettre en œuvre la technique d'encodage à un point en Python en utilisant la méthode get_dummies de pandas pour transformer les variables catégorielles dans un format adapté aux modèles d'apprentissage automatique.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083ee5a1-9167-4f7e-b3b7-27cd36438328",
   "metadata": {},
   "source": [
    "# Analyse exploratoire des données (AED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf3e3f6-0724-47a2-8f8c-988684dc81aa",
   "metadata": {},
   "source": [
    "Dans ce module, nous allons aborder les bases de l'analyse exploratoire des données à l'aide de Python. L'analyse exploratoire des données, ou en bref, EDA, est une approche permettant d'analyser les données afin de résumer les principales caractéristiques des données et de mieux comprendre l'ensemble de données. Découvrez les relations entre les différentes variables et extrayez les variables importantes pour le problème que nous essayons de résoudre. La principale question à laquelle nous essayons de répondre dans ce module est la suivante : quelles sont les caractéristiques qui ont le plus d'impact sur le prix d'une voiture ? Nous passerons en revue quelques techniques exploratoires d'analyse de données utiles afin de répondre à cette question. Dans ce module, vous découvrirez les statistiques descriptives, qui décrivent les caractéristiques de base d'un ensemble de données et obtiennent un bref résumé de l'échantillon et des mesures des données. Principes de base du regroupement de données à l'aide de GroupBy et comment cela peut aider à transformer notre ensemble de données. ANOVA, l'analyse de la variance est une méthode statistique dans laquelle la variation d'un ensemble d'observations est divisée en composantes distinctes. Corrélation entre différentes variables. Enfin, la corrélation avancée, où nous vous présenterons diverses méthodes statistiques de corrélation, à savoir la corrélation de Pearson et les cartes thermiques de corrélation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd70ae0-562d-4029-81f8-eab9815abe40",
   "metadata": {},
   "source": [
    "# Statistiques descriptives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06b5c4-305f-4cdb-bcf1-bbb606cfacf3",
   "metadata": {},
   "source": [
    "nous allons parler de statistiques descriptives. Lorsque vous commencez à analyser des données, il est important d'explorer d'abord vos données avant de passer du temps à créer des modèles complexes. Pour ce faire, vous pouvez facilement calculer des statistiques descriptives pour vos données. L'analyse statistique descriptive permet de décrire les caractéristiques de base d'un ensemble de données et d'obtenir un bref résumé de l'échantillon et des mesures des données. Nous allons vous montrer quelques méthodes utiles. L'une des façons de le faire est d' utiliser la fonction décrite dans les pandas. En utilisant la fonction décrite et en l'appliquant à votre bloc de données, une fonction décrite calcule automatiquement les statistiques de base pour toutes les variables numériques. Il indique la moyenne, le nombre total de points de données, l'écart type, les quartiles et les valeurs extrêmes. Toutes les valeurs NaN sont automatiquement ignorées dans ces statistiques. Cette fonction vous donnera une idée plus précise de la distribution de vos différentes variables. Vous pouvez également avoir des variables catégorielles dans votre ensemble de données. Il s'agit de variables qui peuvent être divisées en différentes catégories ou groupes et qui ont des valeurs discrètes. Par exemple, dans notre ensemble de données, le système d'entraînement est une variable catégorielle, qui comprend les catégories traction avant, traction arrière et quatre roues motrices. Vous pouvez notamment résumer les données catégorielles en utilisant la fonction value_counts. Nous pouvons modifier le nom de la colonne pour en faciliter la lecture. Nous constatons que nous avons 118 voitures dans la catégorie à traction avant, 75 voitures dans la catégorie à traction arrière et huit voitures dans la catégorie à quatre roues motrices. Les diagrammes à cases constituent un excellent moyen de visualiser des données numériques, car vous pouvez visualiser les différentes distributions des données. Les principales caractéristiques illustrées par le diagramme à cases sont la médiane des données qui représente le point de données central, le quartile supérieur indique où se trouve le 75e percentile, le quartile inférieur indique où se trouve le 25e percentile. Les données entre le quartile supérieur et le quartile inférieur représentent l'intervalle interquartile. Ensuite, vous avez les extrêmes inférieurs et supérieurs. Ils sont calculés comme 1,5 fois l'intervalle interquartile au-dessus du 75e percentile et comme 1,5 fois l'IQR inférieur au 25e percentile. Enfin, les diagrammes à cases affichent également les valeurs aberrantes sous forme de points individuels situés en dehors des extrêmes supérieur et inférieur. Avec les diagrammes à cases, vous pouvez facilement repérer les valeurs aberrantes et également voir la distribution et l'asymétrie des données. Les diagrammes à cases facilitent la comparaison entre les groupes. Dans cet exemple, à l'aide d'un diagramme à cases, nous pouvons voir la répartition des différentes catégories de la fonction roues motrices par rapport à la fonction prix. Nous pouvons constater que la répartition du prix entre la traction arrière et les autres catégories est distincte. Mais le prix de la traction avant et de la traction intégrale est presque indissociable. Nous avons souvent tendance à voir des variables continues dans nos données. Ces points de données sont des nombres contenus dans une certaine plage. Par exemple, dans notre jeu de données, le prix et la taille du moteur sont des variables continues. Et si nous voulions comprendre la relation entre la cylindrée du moteur et le prix ? La cylindrée du moteur pourrait-elle prédire le prix d'une voiture ? Un bon moyen de visualiser cela est d'utiliser un diagramme de dispersion. Chaque observation d'un diagramme de points est représentée par un point. Ce graphique montre la relation entre deux variables. La variable prédictive est la variable que vous utilisez pour prévoir un résultat. Dans ce cas, notre variable prédictive est la cylindrée du moteur. La variable cible est la variable que vous essayez de prévoir. Dans ce cas, notre variable cible est le prix puisque ce serait le résultat. Dans un diagramme de dispersion, nous définissons généralement la variable prédictive sur l'axe X ou l'axe horizontal, et nous définissons la variable cible sur l'axe Y ou l'axe vertical. Dans ce cas, nous allons donc tracer la cylindrée du moteur sur l'axe X et le prix sur l'axe Y. Nous utilisons ici la fonction scatter de Matplotlib, en prenant x et une variable y. Il convient de noter qu'il est toujours important d'étiqueter vos haches et d'écrire un titre général de l'intrigue afin de savoir ce que vous regardez. Maintenant, quel est le lien entre la cylindrée variable du moteur et le prix ? Le diagramme de dispersion montre qu'à mesure que la cylindrée du moteur augmente, le prix de la voiture augmente également. Cela nous donne une première indication qu'il existe une relation linéaire positive entre ces deux variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e124eb-0e44-4e26-8139-da1696409c6d",
   "metadata": {},
   "source": [
    "# GroupBy en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7fe79f-fd45-4e5e-ae78-b31f9e44b969",
   "metadata": {},
   "source": [
    "nous aborderons les principes de base du regroupement et la manière dont cela peut contribuer à transformer notre ensemble de données. Supposons que vous vouliez savoir s'il existe un lien entre les différents types de systèmes de propulsion, à traction avant, arrière et quatre roues motrices, et le prix des véhicules. Dans l' affirmative, quel type de système d'entraînement apporte le plus de valeur à un véhicule ? Ce serait bien si nous pouvions regrouper toutes les données selon les différents types de roues motrices et comparer les résultats de ces différentes roues motrices les uns par rapport aux autres. Dans Pandas, cela peut être fait en utilisant la méthode groupby. La méthode groupby est utilisée sur les variables catégorielles, regroupe les données en sous-ensembles en fonction des différentes catégories de cette variable, vous pouvez les regrouper par une seule variable ou vous pouvez les regrouper par plusieurs variables en transmettant plusieurs noms de variables. À titre d'exemple, supposons que nous souhaitions déterminer le prix moyen des véhicules et observer en quoi ils diffèrent selon les types de carrosserie et les variables relatives aux roues motrices. Pour ce faire, nous sélectionnons d'abord les trois colonnes de données qui nous intéressent, ce qui est fait dans la première ligne de code. Nous regroupons ensuite les données réduites en fonction des roues motrices et du style de carrosserie sur la deuxième ligne. Puisque nous voulons savoir comment le prix moyen varie d'un groupe à l'autre, nous pouvons prendre la moyenne de chaque groupe et l' ajouter à ce bit tout à la fin de la ligne 2. Les données sont désormais regroupées en sous-catégories et seul le prix moyen de chaque sous-catégorie est affiché. Nous pouvons constater que, selon nos données, les cabriolets à traction arrière et les toits rigides à traction arrière ont la valeur la plus élevée, tandis que les berlines à quatre roues motrices ont la valeur la plus faible. Un tableau de ce formulaire n'est pas le plus facile à lire et n'est pas très facile à visualiser. Pour faciliter la compréhension, nous pouvons transformer ce tableau en tableau croisé dynamique en utilisant la méthode du pivot. Dans le tableau précédent, les roues motrices et le style de carrosserie étaient répertoriés dans des colonnes. Dans un tableau croisé dynamique, une variable est affichée le long des colonnes et l'autre variable est affichée le long des lignes. Avec une seule ligne de code et en utilisant la méthode de pivotement des Pandas, nous pouvons faire pivoter la variable de style de carrosserie pour qu'elle soit affichée le long des colonnes, et les roues motrices seront affichées le long des lignes. Les données de prix deviennent désormais une grille rectangulaire , plus facile à visualiser. Ceci est similaire à ce qui se fait habituellement dans les feuilles de calcul Excel. Une autre façon de représenter le tableau croisé dynamique consiste à utiliser un diagramme de carte thermique. La carte thermique utilise une grille rectangulaire de données et attribue une intensité de couleur en fonction de la valeur des données aux points de la grille. C'est un excellent moyen de tracer la variable cible sur plusieurs variables et d'obtenir ainsi des indices visuels de la relation entre ces variables et la cible. Dans cet exemple, nous utilisons la méthode pcolor de Pyplot pour tracer une carte thermique et convertir le tableau croisé dynamique précédent en une forme graphique. Nous avons spécifié la palette de couleurs rouge-bleu. Dans le graphique de sortie, chaque type de style de carrosserie est numéroté le long de l'axe X et chaque type de roues motrices est numéroté le long de l'axe Y. Les prix moyens sont tracés avec des couleurs différentes en fonction de leurs valeurs selon la barre de couleur. Nous constatons que la partie supérieure de la carte thermique semble avoir des prix plus élevés dans la partie inférieure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e10d98-3e1a-4fac-af7e-28ce6d5dd505",
   "metadata": {},
   "source": [
    "# Corrélation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a74af80-e9ad-4b5b-abb2-db96758de80a",
   "metadata": {},
   "source": [
    "nous allons parler de la corrélation entre différentes variables. La corrélation est une mesure statistique qui permet de mesurer dans quelle mesure les différentes variables sont interdépendantes. En d'autres termes, lorsque nous examinons deux variables au fil du temps, si une variable change, comment cela affecte-t-il l'évolution de l'autre variable ? Par exemple, on sait que le tabagisme est lié au cancer du poumon, car vous courez un risque plus élevé de développer un cancer du poumon si vous fumez. Dans un autre exemple, il existe une corrélation entre les variables relatives au parapluie et à la pluie, selon lesquelles plus de précipitations signifient que plus de personnes utilisent des parapluies. De plus, s'il ne pleut pas, les gens ne porteront pas de parapluies. On peut donc dire que les parapluies et la pluie sont interdépendants et qu'ils sont par définition corrélés. Il est important de savoir que corrélation ne signifie pas causalité. En fait, on peut dire que le parapluie et la pluie sont corrélés, mais nous n'aurions pas assez d'informations pour dire si le parapluie a causé la pluie ou si c'est la pluie qui a causé le parapluie. En science des données, nous nous intéressons généralement davantage à la corrélation. Examinons la corrélation entre la taille du moteur et le prix. Cette fois, nous allons visualiser ces deux variables à l'aide d'un diagramme de points et d'une droite linéaire ajoutée appelée droite de régression, qui indique la relation entre les deux. L'objectif principal de ce graphique est de voir si la cylindrée du moteur a un impact sur le prix. Dans cet exemple, vous pouvez constater que la ligne droite passant par les points de données est très raide, ce qui montre qu'il existe une relation linéaire positive entre les deux variables. Avec l'augmentation des valeurs de la cylindrée du moteur, les valeurs du prix augmentent également et la pente de la ligne est positive. Il existe donc une corrélation positive entre la taille du moteur et le prix. Nous pouvons utiliser le diagramme Seaborne Reg pour créer le diagramme de dispersion. Comme autre exemple, examinons maintenant la relation entre les miles autoroutiers par gallon pour voir son impact sur le prix de la voiture. Comme nous pouvons le voir sur ce graphique, lorsque la valeur des miles autoroutiers par gallon augmente, la valeur du prix diminue. Il existe donc une relation linéaire négative entre le nombre de miles autoroutiers par gallon et le prix. Bien que cette relation soit négative, la pente de la ligne est abrupte, ce qui signifie que le nombre de kilomètres parcourus par l'autoroute par gallon reste un bon indicateur du prix. Ces deux variables sont considérées comme présentant une corrélation négative. Enfin, nous avons un exemple de faible corrélation. Par exemple, les faibles niveaux de régime de pointe et les valeurs élevées de régime de pointe se traduisent par des prix bas et élevés. Par conséquent, nous ne pouvons pas utiliser le RPM pour prédire les valeurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820ea59-d3c0-45c1-beed-63e821c6abc7",
   "metadata": {},
   "source": [
    "# Corrélation - Statistiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b77bac5-5377-4cef-aa65-2288e281aeb9",
   "metadata": {},
   "source": [
    "nous allons vous présenter différentes méthodes statistiques de corrélation. L'un des moyens de mesurer la force de la corrélation entre des variables numériques continues consiste à utiliser une méthode appelée corrélation de Pearson. La méthode de corrélation de Pearson vous donnera deux valeurs : le coefficient de corrélation et la valeur p. Comment interprétons-nous ces valeurs ? Pour le coefficient de corrélation, une valeur proche de un implique une forte corrélation positive, tandis qu'une valeur proche de -1 implique une forte corrélation négative, et une valeur proche de zéro implique l'absence de corrélation entre les variables. Ensuite, la valeur de p nous indiquera dans quelle mesure nous sommes certains de la corrélation que nous avons calculée. Pour la valeur p, une valeur inférieure à 0,001 nous donne une forte certitude quant au coefficient de corrélation que nous avons calculé, une valeur comprise entre 0,001 et 0,05 nous donne une certitude modérée, une valeur comprise entre 0,05 et 0,1 nous donne une faible certitude, et une valeur p supérieure à 0,1 ne nous donne aucune certitude de corrélation. On peut dire qu'il existe une forte corrélation lorsque le coefficient de corrélation est proche de un ou -1 et que la valeur de p est inférieure à 0,001. Le graphique suivant montre des données avec différentes valeurs de corrélation. Dans cet exemple, nous voulons examiner la corrélation entre les variables puissance et prix de la voiture. Découvrez à quel point vous pouvez facilement calculer la corrélation de Pearson à l'aide du package de statistiques Scipy ? Nous pouvons voir que le coefficient de corrélation est d'environ 0,8 et qu'il est proche de un, il y a donc une forte corrélation positive. Nous pouvons également constater que la valeur de p est très faible, bien inférieure à 0,001, et nous pouvons donc en conclure que nous sommes certains de la forte corrélation positive. En tenant compte de toutes les variables, nous pouvons désormais créer une carte thermique qui indique\n",
    "Lisez la vidéo à partir de :2:4 et suivez la transcription2:04\n",
    "la corrélation entre chacune des variables. La palette de couleurs indique le coefficient de corrélation de Pearson, qui indique la force de la corrélation entre deux variables. Nous pouvons voir une diagonale rouge foncé indiquant que toutes les valeurs de cette diagonale sont fortement corrélées. Cela est logique, car lorsque vous regardez de plus près, les valeurs sur la diagonale sont la corrélation de toutes les variables avec elles-mêmes, qui seront toujours une. Cette carte thermique de corrélation nous donne un bon aperçu de la façon dont les différentes variables sont liées les unes aux autres, et surtout, de la manière dont ces variables sont liées au prix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029bd756-0e5a-4517-87e9-db9e74ed40af",
   "metadata": {},
   "source": [
    "# Résumé de la leçonStatut : Traduit automatiquement de Anglais\n",
    "Traduit automatiquement de Anglais\n",
    "Nous vous félicitons ! Vous avez terminé cette leçon. À ce stade du cours, vous savez ce qui suit :\n",
    "\n",
    "Des outils tels que la fonction \"describe\" de pandas permettent de calculer rapidement des mesures statistiques clés telles que la moyenne, l'écart type et les quartiles pour toutes les variables numériques de votre cadre de données.\n",
    "\n",
    "Utilisez la fonction \"value_counts\" pour résumer les données en différentes catégories pour les données catégorielles.\n",
    "\n",
    "Les diagrammes en boîte offrent une représentation plus visuelle de la distribution des données numériques, en indiquant des caractéristiques telles que la médiane, les quartiles et les valeurs aberrantes.\n",
    "\n",
    "Les diagrammes de dispersion sont excellents pour explorer les relations entre les variables continues, telles que la taille du moteur et le prix, dans un ensemble de données automobiles.\n",
    "\n",
    "Utilisez la méthode \"groupby\" de Pandas pour explorer les relations entre les variables catégorielles.\n",
    "\n",
    "Utilisez les tableaux croisés dynamiques et les cartes thermiques pour une meilleure visualisation des données.\n",
    "\n",
    "La corrélation entre les variables est une mesure statistique qui indique comment les changements d'une variable peuvent être associés aux changements d'une autre variable.\n",
    "\n",
    "Lorsque vous étudiez la corrélation, utilisez des diagrammes de dispersion combinés à une ligne de régression pour visualiser les relations entre les variables.\n",
    "\n",
    "Les fonctions de visualisation telles que regplot, de la bibliothèque seaborn , sont particulièrement utiles pour explorer la corrélation.\n",
    "\n",
    "La corrélation de Pearson, une méthode clé pour évaluer la corrélation entre des variables numériques continues, fournit deux valeurs critiques : le coefficient, qui indique la force et la direction de la corrélation, et la valeur P, qui évalue la certitude de la corrélation.\n",
    "\n",
    "Un coefficient de corrélation proche de 1 ou de -1 indique une forte corrélation positive ou négative, respectivement, tandis qu'un coefficient proche de zéro indique l'absence de corrélation.\n",
    "\n",
    "Pour les valeurs P, les valeurs inférieures à 0,001 indiquent une forte certitude dans la corrélation, tandis que les valeurs plus élevées indiquent une certitude moindre. Le coefficient et la valeur P sont tous deux importants pour confirmer une forte corrélation.\n",
    "\n",
    "Les cartes thermiques fournissent un résumé visuel complet de la force et de la direction des corrélations entre plusieurs variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502d8350-a537-410d-8b8c-ad900bec9ca6",
   "metadata": {},
   "source": [
    "# Développement de modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ca7b7-bb48-44bb-b6ba-e44e92b01dec",
   "metadata": {},
   "source": [
    "nous allons examiner le développement de modèles en essayant de prédire le prix d'une voiture à l'aide de notre jeu de données. Dans ce module, vous découvrirez la régression linéaire simple et multiple, l' évaluation de modèles à l'aide de la visualisation, la régression polynomiale et les pipelines, le R carré et le MSE pour l'évaluation, la prévision et la prise de décision dans l'échantillon, et comment déterminer la juste valeur d'une voiture d'occasion. Un modèle ou un estimateur peut être considéré comme une équation mathématique utilisée pour prédire une valeur à partir d'une ou de plusieurs autres valeurs, en reliant une ou plusieurs variables ou caractéristiques indépendantes à des variables dépendantes. Par exemple, vous saisissez les miles autoroutiers par gallon d'un modèle de voiture en tant que variable ou fonction indépendante. Le résultat du modèle ou de la variable dépendante est le prix. En général, plus vous disposez de données pertinentes, plus votre modèle est précis. Par exemple, vous entrez plusieurs variables ou fonctions indépendantes dans votre modèle. Par conséquent, votre modèle peut prévoir un prix plus précis pour le véhicule. Pour comprendre pourquoi il est important de disposer de davantage de données, considérez la situation suivante. Vous avez deux voitures presque identiques. Les voitures roses se vendent beaucoup moins cher. Vous souhaitez utiliser votre modèle pour déterminer le prix de deux voitures, l'une rose, l'autre rouge. Si les variables ou caractéristiques indépendantes de votre modèle n'incluent pas la couleur, celui-ci prédira le même prix pour les voitures susceptibles de se vendre bien moins cher. En plus d'obtenir davantage de données, vous pouvez essayer différents types de modèles. Dans ce cours, vous découvrirez la régression linéaire simple, la régression linéaire multiple et la régression polynomiale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8786fe45-d85e-462c-8914-b000266e63bb",
   "metadata": {},
   "source": [
    "# Régression linéaire et régression linéaire multiple#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2266d-792d-4808-87ef-3175979c8874",
   "metadata": {},
   "source": [
    "nous allons parler de régression linéaire simple et de régression linéaire multiple. La régression linéaire fera référence à une variable indépendante pour établir une prédiction. La régression linéaire multiple fera référence à plusieurs variables indépendantes pour établir une prédiction. La régression linéaire simple, ou SLR, est une méthode qui nous aide à comprendre la relation entre deux variables : le prédicteur (variable indépendante x) et la cible (variable dépendante y). Nous aimerions établir une relation linéaire entre les variables présentées ici. Le paramètre b0 est l'intersection, le paramètre b1 est la pente. Lorsque nous ajustons ou entraînons le modèle, nous définissons ces paramètres. Cette étape nécessite beaucoup de mathématiques, nous ne nous concentrerons donc pas sur cette partie. Clarifions l'étape de prédiction. Il est difficile de savoir combien coûte une voiture, mais le nombre de kilomètres parcourus par l'autoroute par gallon figure dans le manuel du propriétaire. Si nous supposons qu'il existe une relation linéaire entre ces variables, nous pouvons utiliser cette relation pour formuler un modèle afin de déterminer le prix de la voiture. Si le nombre de miles autoroutiers par gallon est de 20, nous pouvons saisir cette valeur dans le modèle pour obtenir une prévision de 22 000$. Afin de déterminer la ligne, nous prenons les points de données de notre ensemble de données marqués en rouge ici. Nous utilisons ensuite ces points d'entraînement pour adapter notre modèle. Les résultats des points d'entraînement sont les paramètres. Nous stockons généralement les points de données dans deux trames de données ou dans des tableaux NumPy. La valeur que nous aimerions prédire est appelée la cible que nous stockons dans le tableau y. Nous stockons la variable dépendante dans le cadre de données ou le tableau x. Chaque échantillon correspond à une ligne différente dans chaque trame de données ou tableau. Dans de nombreux cas, de nombreux facteurs influencent le montant que les gens ont payé pour une voiture, par exemple la marque ou l'âge de la voiture. Dans ce modèle, cette incertitude est prise en compte en supposant qu' une petite valeur aléatoire est ajoutée au point de la ligne. C'est ce qu'on appelle le bruit. La figure de gauche montre la distribution du bruit. L'axe vertical indique la valeur ajoutée et l'axe horizontal illustre la probabilité que la valeur soit ajoutée. Habituellement, une petite valeur positive est ajoutée ou une petite valeur négative.\n",
    "Lisez la vidéo à partir de :2:30 et suivez la transcription2:30\n",
    "Parfois, de grandes valeurs sont ajoutées, mais dans la plupart des cas, les valeurs ajoutées sont proches de zéro.\n",
    "Lisez la vidéo à partir de :2:39 et suivez la transcription2:39\n",
    "Nous pouvons résumer le processus comme suit. Nous avons un ensemble de points d'entraînement. Nous utilisons ces points d'entraînement pour ajuster ou entraîner le modèle et obtenir des paramètres. Nous utilisons ensuite ces paramètres dans le modèle, nous avons maintenant un modèle. Nous utilisons le chapeau sur le Y pour indiquer que le modèle est une estimation. Nous pouvons utiliser ce modèle pour prévoir des valeurs que nous n'avons pas vues. Par exemple, nous n'avons aucune voiture parcourant 20 miles par gallon sur autoroute. Nous pouvons utiliser notre modèle pour prédire le prix de cette voiture, mais n'oubliez pas que notre modèle n'est pas toujours correct. Nous pouvons le constater en comparant la valeur prévue à la valeur réelle. Nous avons un échantillon de dix miles autoroutiers par gallon, mais la valeur prévue ne correspond pas à la valeur réelle. Si l'hypothèse linéaire est correcte, cette erreur est due au bruit, mais il peut y avoir d'autres raisons. Pour adapter le modèle en Python, nous importons d'abord le modèle linéaire depuis scikit-learn. Créez ensuite un objet de régression linéaire à l'aide du constructeur. Nous définissons la variable prédictive et la variable cible. Utilisez ensuite la méthode fit pour ajuster le modèle et trouvez les paramètres b0 et b1, les entrées étant les entités et les cibles. Nous pouvons obtenir une prédiction en utilisant la méthode predict. Le résultat est un tableau. Le tableau contient le même nombre d'échantillons que l'entrée x. L'intersection b0 est un attribut de l'objet lm, la pente b1 est également un attribut de l'objet lm.\n",
    "Lisez la vidéo à partir de :4:20 et suivez la transcription4:20\n",
    "La relation entre le prix et les miles autoroutiers par gallon est donnée par cette équation en gras : prix = 38 423,31 - 821,73 fois le nombre de miles autoroutiers par gallon, comme l'équation dont nous avons parlé précédemment.\n",
    "Lisez la vidéo à partir de :4:38 et suivez la transcription4:38\n",
    "La régression linéaire multiple est utilisée pour expliquer la relation entre une variable cible continue (y) et au moins deux variables prédictives (x).\n",
    "Lisez la vidéo à partir de :4:49 et suivez la transcription4:49\n",
    "Si nous avons, par exemple, quatre variables prédictives, alors b0 intercepte x=0, b1, le coefficient ou le paramètre de x1, b2, le coefficient du paramètre x2, etc. S'il n'y a que deux variables, nous pouvons visualiser les valeurs. Considérez la fonction suivante : les variables x1 et x2 peuvent être visualisées sur un plan 2D. Prenons un exemple sur la diapositive suivante. Le tableau contient différentes valeurs des variables prédictives x1 et x2. La position de chaque point est placée sur le plan 2D et codée par couleur en conséquence. Chaque valeur des variables prédictives x1 et x2 sera mappée à une nouvelle valeur y, y hat. Les nouvelles valeurs de y, y hat, sont mappées dans le sens vertical avec une hauteur proportionnelle à la valeur que prend y hat.\n",
    "Lisez la vidéo à partir de :5:48 et suivez la transcription5:48\n",
    "Nous pouvons ajuster la régression linéaire multiple comme suit. Nous pouvons extraire les quatre variables prédictives et les stocker dans la variable z, puis entraîner le modèle comme auparavant en utilisant l'ajustement de la méthode ou les variables dépendantes et les deux points cibles. Nous pouvons également obtenir une prédiction en utilisant la méthode predict. Dans ce cas, l'entrée est un tableau ou un bloc de données à quatre colonnes. Le nombre de lignes correspond au nombre d'échantillons. Le résultat est un tableau contenant le même nombre d'éléments que le nombre d'échantillons. L'interception est un attribut de l'objet et les coefficients sont également des attributs. Il est utile de visualiser l'équation en remplaçant les noms des variables dépendantes par des noms réels. Il s'agit d'un formulaire identique à celui dont nous avons parlé plus haut."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e03b5-e4a1-4f48-9b9a-4df6ba602668",
   "metadata": {},
   "source": [
    "# Évaluation des modèles à l'aide de la visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34a7fb-4f7d-42ca-b0fd-28d634717315",
   "metadata": {},
   "source": [
    "nous allons examiner l'évaluation du modèle à l'aide de la visualisation. Les diagrammes de régression sont une bonne estimation de la relation entre deux variables, de la force de la corrélation et de l'orientation de la relation, positive ou négative. L'axe horizontal est la variable indépendante. L'axe vertical est la variable dépendante. Chaque point représente un point cible différent. La ligne ajustée représente la valeur prédite. Il existe plusieurs méthodes pour tracer un diagramme de régression. Une méthode simple consiste à utiliser regplot depuis la bibliothèque Seaborn. Importez d'abord Seaborn, puis utilisez la fonction regplot. Le paramètre x est le nom de la colonne contenant la variable ou la fonction indépendante. Le paramètre y contient le nom de la colonne contenant le nom de la variable dépendante ou de la cible. Les données du paramètre sont le nom de la trame de données. Le résultat est donné par le graphique. Le graphique des valeurs résiduelles représente l'erreur entre la valeur réelle. En examinant la valeur prévue et la valeur réelle, nous constatons une différence. Nous avons obtenu cette valeur en soustrayant la valeur prévue de la valeur cible réelle. Nous traçons ensuite cette valeur sur l'axe vertical avec la variable dépendante comme axe horizontal. De même, pour le deuxième échantillon, nous répétons le processus en soustrayant la valeur cible de la valeur prédite, puis en traçant la valeur en conséquence. L'examen du graphique nous donne un aperçu de nos données. Nous nous attendons à ce que les résultats aient une moyenne nulle répartie uniformément autour de l'axe X avec une variance similaire. Il n'y a aucune courbure. Ce type de diagramme des valeurs résiduelles suggère qu'un graphique linéaire est approprié. Dans ce graphique résiduel, il y a une courbure. Les valeurs de l'erreur changent avec x. Par exemple, dans la région, toutes les erreurs résiduelles sont positives. Dans cette zone, les valeurs résiduelles sont négatives. À l'emplacement final, l'erreur est importante. Les valeurs résiduelles ne sont pas séparées de manière aléatoire. Cela suggère que l'hypothèse linéaire est incorrecte. Ce graphique suggère une fonction non linéaire. Nous aborderons cette question dans la section suivante. Dans ce graphique, nous voyons que la variance des valeurs résiduelles augmente avec x. Notre modèle est donc incorrect. Nous pouvons utiliser Seaborn pour créer un graphique résiduel. Tout d' abord, importez Seaborn, nous utilisons la fonction residplot.\n",
    "Lisez la vidéo à partir de :2:40 et suivez la transcription2:40\n",
    "Le premier paramètre est une série de variables ou de fonctionnalités dépendantes. Le deuxième paramètre est une série de variables dépendantes ou de cibles. Nous voyons dans ce cas que les valeurs résiduelles ont une courbure.\n",
    "Lisez la vidéo à partir de :2:53 et suivez la transcription2:53\n",
    "Un diagramme de distribution compte la valeur prévue par rapport à la valeur réelle. Ces diagrammes sont extrêmement utiles pour visualiser des modèles comportant plusieurs variables ou caractéristiques indépendantes. Regardons un exemple simplifié. Nous examinons l'axe vertical. Nous comptons et traçons ensuite le nombre de points prédits approximativement égaux à un. Nous comptons et traçons ensuite le nombre de points prédits approximativement égaux à deux. Nous répétons le processus pour les points prédits approximativement égaux à trois. Ensuite, nous répétons le processus pour les valeurs cibles. Dans ce cas, toutes les valeurs cibles sont approximativement égales à deux. Les valeurs sont les cibles et les valeurs prédites sont continues. Un histogramme est destiné aux valeurs discrètes. Les Pandas les convertiront donc en distribution. L'axe vertical est redimensionné pour que la zone située sous la distribution soit égale à un. Voici un exemple d'utilisation d'un diagramme de distribution. La variable ou caractéristique dépendante est le prix. Les valeurs ajustées qui résultent du modèle sont en bleu. Les valeurs réelles sont en rouge. Nous constatons que les valeurs prévues pour des prix compris entre 40 000 et 50 000 sont inexactes. Les prix dans la région, compris entre 10 000 et 20 000, sont beaucoup plus proches de la valeur cible. Dans cet exemple, nous utilisons plusieurs caractéristiques ou variables indépendantes pour le comparer au graphique de la dernière diapositive. Nous constatons que les valeurs prédites sont beaucoup plus proches des valeurs cibles. Voici le code pour créer un diagramme de distribution. Les valeurs réelles sont utilisées comme paramètre. Nous voulons une distribution plutôt qu'un histogramme. Nous voulons donc que les paramètres hist soient définis sur false. La couleur est rouge. L'étiquette est également incluse. Les valeurs prédites sont incluses pour le second graphique. Les autres paramètres sont définis en conséquence. [MUSIQUE]\n",
    "fr\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acedba5e-6f45-46ca-a7f2-72c33d02f74f",
   "metadata": {},
   "source": [
    " # Mesures pour l'évaluation en échantillon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596bcbda-8e7a-4461-b352-fc3c0fa90c12",
   "metadata": {},
   "source": [
    "Maintenant que nous avons vu comment évaluer un modèle à l'aide de la visualisation, nous voulons évaluer nos modèles numériquement. Examinons certaines des mesures que nous utilisons pour l'évaluation sur échantillon. Ces mesures permettent de déterminer numériquement dans quelle mesure le modèle s'adapte à nos données. Deux mesures importantes que nous utilisons souvent pour déterminer l'ajustement d' un modèle sont la moyenne au carré, l'erreur, le MSE et le R-carré. Pour mesurer le MSE, nous trouvons la différence entre la valeur réelle y et la valeur prédite y Hat puis la mettons au carré. Dans ce cas, la valeur réelle est de 150. La valeur prédite est de 50. En soustrayant ces points, nous obtenons 100. Nous mettons ensuite le nombre au carré. Nous prenons ensuite la moyenne de toutes les erreurs en les additionnant et en les divisant par le nombre d'échantillons. Pour trouver le MSE en Python, nous pouvons importer le mean_squared_error depuis sk.learn.metrics. La fonction mean_squared_error obtient deux entrées, la valeur réelle de la variable cible et la valeur prédite de la variable cible. R au carré est également appelé coefficient de détermination. Il s'agit d'une mesure qui permet de déterminer à quel point les données sont proches de la droite de régression ajustée. Dans quelle mesure nos données réelles sont-elles proches de notre modèle estimé ? Pensez-y comme si vous compariez un modèle de régression à un modèle simple , c'est-à-dire la moyenne des points de données. Si la variable x est un bon prédicteur, notre modèle devrait être bien plus performant que la simple moyenne. Dans cet exemple, la moyenne des points de données y bar est de six. Le coefficient de détermination R au carré est égal à un moins le ratio entre le MSE de la droite de régression et le MSE de la moyenne des points de données. Pour la plupart, il prend des valeurs comprises entre zéro et un. Regardons un cas où la ligne offre un ajustement relativement bon. La ligne bleue représente la droite de régression. Les carrés bleus représentent le MSE de la droite de régression. La ligne rouge représente la valeur moyenne des points de données. Les carrés rouges représentent le MSE de la ligne rouge. Nous voyons que l'aire des carrés bleus est beaucoup plus petite que celle des carrés rouges. Dans ce cas, comme la droite est bien ajustée, l'erreur quadratique moyenne est faible. Le numérateur est donc petit. L'erreur quadratique moyenne de la moyenne des données est importante car le dénominateur est grand. Un petit nombre divisé par un plus grand nombre est un nombre encore plus petit. Poussée à l'extrême, cette valeur tend vers zéro. Si nous saisissons cette valeur de la diapositive précédente pour R au carré, nous obtenons une valeur proche de un. Cela signifie que la ligne est bien adaptée aux données. Voici un exemple de ligne qui ne correspond pas bien aux données. Si nous examinons simplement l'aire des carrés rouges par rapport aux carrés bleus, nous voyons que l'aire est presque identique. Le ratio des surfaces est proche de un. Dans ce cas, le R au carré est proche de zéro. Cette ligne fonctionne à peu près de la même manière que si vous utilisiez simplement la moyenne des points de données. Par conséquent, cette ligne n'a pas donné de bons résultats. Nous trouvons la valeur R au carré en Python en utilisant la méthode du score dans l' objet de régression linéaire. D'après la valeur que nous obtenons de cet exemple, nous pouvons dire qu'environ 49,659 % de la variation de prix s'explique par ce modèle linéaire simple. Votre valeur R au carré est généralement comprise entre zéro et un. Si votre carré r est négatif, cela peut être dû à un surajustement dont nous parlerons dans le module suivant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dafd012-7dc8-42ee-a632-407957fadc9d",
   "metadata": {},
   "source": [
    "# Prédiction et prise de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50be81a-339a-40f1-8b64-f77772064bd1",
   "metadata": {},
   "source": [
    "notre dernier sujet portera sur les prévisions et la prise de décision. Comment déterminer si notre modèle est correct ? La première chose à faire est de vous assurer que les résultats de votre modèle sont pertinents. Vous devez toujours utiliser la visualisation, les mesures numériques pour l'évaluation et la comparaison entre les différents modèles. Regardons un exemple de prédiction. Si vous vous souvenez, nous entraînons le modèle en utilisant la méthode d'ajustement. Nous voulons maintenant savoir quel serait le prix d' une voiture parcourant 30 miles par gallon sur l'autoroute. L'intégration de cette valeur dans la méthode de prédiction nous donne un prix obtenu de 13 771,30$ \n",
    "\n",
    "\n",
    ". Cela semble logique. Par exemple, la valeur n'est ni négative, ni extrêmement élevée, ni extrêmement basse. Nous pouvons examiner les coefficients en examinant l'attribut coef_. Si vous vous souvenez, l'expression du modèle linéaire simple qui prédit le prix en fonction des miles autoroutiers par gallon correspond au multiple de la caractéristique des miles autoroutiers par gallon. Ainsi, une augmentation d' une unité de miles autoroutiers par gallon entraîne une diminution d'environ 821$ de la valeur de la voiture. Cette valeur semble également raisonnable. Parfois, votre modèle produit des valeurs qui n'ont aucun sens. Par exemple, si nous traçons le modèle pour les miles autoroutiers par gallon compris entre 0 et 100, nous obtenons des valeurs négatives pour le prix. Cela peut être dû au fait que les valeurs de cette plage ne sont pas réalistes. L'hypothèse linéaire est incorrecte ou nous ne disposons pas de données pour les voitures de cette gamme. Dans ce cas, il est peu probable qu' une voiture ait une consommation de carburant dans cette fourchette, notre modèle semble donc valide. Pour générer une séquence de valeurs dans une plage spécifiée, importez NumPy, puis utilisez la fonction NumPy a range pour générer la séquence. La séquence commence à un et augmente d'une unité jusqu'à atteindre 100. Le premier paramètre est le point de départ de la séquence. Le deuxième paramètre est le point final plus un élément de la séquence. Le dernier paramètre est la taille du pas entre les éléments de la séquence. Dans ce cas, c'en est un. Nous incrémentons la séquence étape par étape, de 1 à 2 et ainsi de suite. Nous pouvons utiliser le résultat pour prévoir de nouvelles valeurs. La sortie est un tableau NumPy, la plupart des valeurs sont négatives.\n",
    "\n",
    "\n",
    "Lapremière méthode que vous devriez essayer est d'utiliser un diagramme de régression pour visualiser vos données. Consultez les laboratoires pour obtenir des exemples de diagrammes de régression polynomiale. Dans cet exemple, l'effet de la variable indépendante est évident dans ce cas. Les données tendent à diminuer à mesure que la variable dépendante augmente. Le graphique montre également un certain comportement non linéaire. En examinant le graphique des valeurs résiduelles, nous constatons que dans ce cas, les valeurs résiduelles ont une courbure, ce qui suggère un comportement non linéaire. Un diagramme de distribution est une bonne méthode de régression linéaire multiple. Par exemple, nous constatons que les valeurs prédites pour des prix compris entre 30 000 et 50 000 sont inexactes. Cela suggère qu'un modèle non linéaire peut être plus approprié ou que nous avons besoin de plus de données dans cette plage. L'erreur quadratique moyenne est peut-être la mesure numérique la plus intuitive pour déterminer si un modèle est bon ou non. Voyons comment les différentes mesures de l' erreur quadratique moyenne influent sur le modèle. La figure montre un exemple d' erreur quadratique moyenne de 3 495. Cet exemple présente une erreur quadratique moyenne de 3 652. Le graphique final présente une erreur quadratique moyenne de 12 870. À mesure que l'erreur quadratique augmente, les cibles s'éloignent des points prévus. Comme nous en avons discuté, R^2 est une autre méthode populaire pour évaluer votre modèle. Dans ce graphique, nous voyons les points cibles en rouge et la ligne prédite en bleu, ainsi que R^2 de 0,9986. Le modèle semble bien adapté. Ce modèle a un R^2 de 0,9226, il existe toujours une forte relation linéaire. R^2 de 0,806, les données sont beaucoup plus compliquées, mais la relation linéaire est évidente. R^2 de 0,61, la fonction linéaire est plus difficile à voir, mais en y regardant de plus près, nous constatons que les données augmentent avec la variable indépendante. Une valeur acceptable pour R^2 dépend du domaine que vous étudiez. Certains auteurs suggèrent qu'une valeur doit être égale ou supérieure à 0,10. La comparaison entre MLR et SLR est un MSE inférieur, ce qui implique toujours un meilleur ajustement ? Pas nécessairement. Le MSE d'un modèle MLR sera inférieur au MSE d'un modèle SLR, car les erreurs des données diminueront lorsque davantage de variables seront incluses dans le modèle. La régression polynomiale aura également un MSE inférieur à celui de la régression normale. Une relation inverse similaire est valable pour R^2. Dans la section suivante, nous verrons de meilleures façons d'évaluer le modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aae1e5-af4b-4460-8d19-5306bf1e10b1",
   "metadata": {},
   "source": [
    "Résumé de la leçonStatut : Traduit automatiquement de Anglais\n",
    "Traduit automatiquement de Anglais\n",
    "Nous vous félicitons ! Vous avez terminé cette leçon. À ce stade du cours, vous savez :\n",
    "\n",
    "La régression linéaire consiste à utiliser une variable indépendante pour faire une prédiction.\n",
    "\n",
    "Vous pouvez utiliser la régression linéaire multiple pour expliquer la relation entre une variable cible continue y et deux variables prédictives x ou plus.\n",
    "\n",
    "La régression linéaire simple, ou SLR, est une méthode utilisée pour comprendre la relation entre deux variables, la variable indépendante prédicteur x et la variable dépendante cible y.\n",
    "\n",
    "Utilisez les fonctions regplot et residplot de la bibliothèque Seaborn pour créer des graphiques de régression et de résidus, qui vous aideront à identifier la force, la direction et la linéarité de la relation entre vos variables indépendantes et dépendantes.\n",
    "\n",
    "Lorsque vous utilisez les tracés de résidus pour l'évaluation du modèle, les résidus doivent idéalement avoir une moyenne nulle, être répartis uniformément autour de l'axe des x et avoir une variance constante. Si ces conditions ne sont pas remplies, envisagez d'ajuster votre modèle.\n",
    "\n",
    "Utilisez les diagrammes de distribution pour les modèles à caractéristiques multiples : Apprenez à construire des diagrammes de distribution pour comparer les valeurs prédites et réelles, en particulier lorsque votre modèle comprend plus d'une variable indépendante. Sachez que cela peut vous donner une idée plus précise de la précision de votre modèle sur différentes plages de valeurs.\n",
    "\n",
    "L'ordre des polynômes affecte l'adéquation du modèle à vos données. Appliquez la fonction polyfit de Python pour développer des modèles de régression polynomiale adaptés à votre ensemble de données spécifique.\n",
    "\n",
    "Pour préparer vos données à une modélisation plus précise, utilisez des techniques de transformation des caractéristiques, en particulier en utilisant la bibliothèque de prétraitement de scikit-learn, transformez vos données en utilisant des caractéristiques polynomiales et utilisez des modules tels que StandardScaler pour normaliser les données.\n",
    "\n",
    "Les pipelines vous permettent de simplifier la manière dont vous effectuez les transformations et les prédictions de manière séquentielle, et vous pouvez utiliser les pipelines dans scikit-learn pour rationaliser votre processus de modélisation.\n",
    "\n",
    "Vous pouvez construire et former un pipeline pour automatiser des tâches telles que la normalisation, la transformation polynomiale et la réalisation de prédictions.\n",
    "\n",
    "Pour déterminer l'adéquation de votre modèle, vous pouvez effectuer des évaluations d'échantillons en utilisant l'erreur quadratique moyenne (MSE), en utilisant la fonction Python mean_squared_error de scikit-learn, et en utilisant la méthode du score pour obtenir la valeur R-carré.\n",
    "\n",
    "Un modèle dont la valeur R au carré est élevée et proche de 1 et dont l'EQM est faible est généralement bien adapté, tandis qu'un modèle dont la valeur R au carré est faible et dont l'EQM est élevée peut ne pas être utile.\n",
    "\n",
    "Soyez attentif aux situations où votre valeur R-carré peut être négative, ce qui peut indiquer un surajustement.\n",
    "\n",
    "Lors de l'évaluation des modèles, utilisez la visualisation et les mesures numériques et comparez différents modèles.\n",
    "\n",
    "L'erreur quadratique moyenne est peut-être la mesure numérique la plus intuitive pour déterminer si un modèle est bon.\n",
    "\n",
    "Un diagramme de distribution est une méthode appropriée pour la régression linéaire multiple.\n",
    "\n",
    "Une valeur acceptable de r-carré dépend de ce que vous étudiez et de votre cas d'utilisation.\n",
    "\n",
    "Pour évaluer l'adéquation de votre modèle, utilisez la visualisation, des méthodes telles que les diagrammes de régression et de résidus, ainsi que des mesures numériques telles que les coefficients de sensibilité du modèle :\n",
    "\n",
    "Utilisez l'erreur quadratique moyenne (EQM) pour mesurer la moyenne des carrés des erreurs entre les valeurs réelles et prédites et examinez le R-carré pour comprendre la proportion de la variance de la variable dépendante qui est prévisible à partir des variables indépendantes.\n",
    "\n",
    "Lors de l'analyse des graphes résiduels, les résidus doivent être distribués de manière aléatoire autour de zéro pour un bon modèle. En revanche, une courbe résiduelle ou des imprécisions dans certaines plages suggèrent un comportement non linéaire ou la nécessité de disposer de davantage de données.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2967318c-2103-4d13-aa34-5f8a0aaa0d23",
   "metadata": {},
   "source": [
    "# Évaluation et affinement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1d1a2-6466-4447-96c1-da28877eb4ca",
   "metadata": {},
   "source": [
    "L'évaluation du modèle nous indique comment notre modèle fonctionne dans le monde réel. Dans le module précédent, nous avons parlé de l'évaluation dans l'échantillon. L'évaluation intégrée à l'échantillon nous indique dans quelle mesure notre modèle s'adapte aux données déjà fournies pour l'entraîner. Cela ne nous donne pas d'estimation de la capacité du modèle entraîné à prédire les nouvelles données. La solution consiste à diviser nos données, à utiliser les données de l'échantillon ou les données d'entraînement pour entraîner le modèle. Le reste des données, appelé données de test, est utilisé comme données hors échantillon. Ces données sont ensuite utilisées pour évaluer approximativement les performances du modèle dans le monde réel. La séparation des données en ensembles de formation et de test est un élément important de l'évaluation des modèles. Nous utilisons les données de test pour avoir une idée des performances de notre modèle dans le monde réel. Lorsque nous divisons un ensemble de données, la plus grande partie des données est généralement utilisée pour l' entraînement et une plus petite partie est utilisée pour les tests. Par exemple, nous pouvons utiliser 70 % des données pour la formation. Nous utilisons ensuite 30 % pour les tests. Nous utilisons un ensemble de formation pour créer un modèle et découvrir des relations prédictives. Nous utilisons ensuite un ensemble de tests pour évaluer les performances du modèle. Lorsque nous aurons terminé de tester notre modèle, nous devrions utiliser toutes les données pour l'entraîner. Une fonction populaire du package Scikit-learn pour diviser des ensembles de données est la fonction train_test_split. Cette fonction divise de manière aléatoire un ensemble de données en sous-ensembles d'entraînement et de test à partir de l'extrait de code d'exemple. Cette méthode est importée depuis la validation de sklearn.cross. Le paramètre d'entrée y_data est la variable cible. Dans l'exemple d'évaluation d'une voiture, il s'agirait du prix, et x_data, de la liste des variables prédictives. Dans ce cas, ce sont toutes les autres variables du jeu de données sur les voitures que nous utilisons pour essayer de prédire le prix. La sortie est un tableau, x_train et y_train. Les sous-ensembles pour l'entraînement x_test, et y_test les sous-ensembles pour les tests. Dans ce cas, la taille du test est un pourcentage des données du kit de test. Ici, c'est 30 %. L'état aléatoire est une graine aléatoire pour le découpage aléatoire des ensembles de données. L'erreur de généralisation est une mesure de la capacité de nos données à prédire des données inédites. L'erreur que nous obtenons en utilisant nos données de test est une approximation de cette erreur. Cette figure montre la distribution des valeurs réelles en rouge par rapport aux valeurs prédites à partir d'une régression linéaire en bleu. Nous constatons que les distributions sont quelque peu similaires. Si nous générons le même graphique en utilisant les données de test, nous constatons que les distributions sont relativement différentes. La différence est due à une erreur de généralisation et représente ce que nous voyons dans le monde réel. L'utilisation d'un grand nombre de données pour l'entraînement nous donne un moyen précis de déterminer les performances de notre modèle dans le monde réel, mais la précision des performances sera faible. Clarifions-le à l'aide d'un exemple. Le centre de cet œil de bœuf représente l'erreur de généralisation correcte. Supposons que nous prenions un échantillon aléatoire des données en utilisant 90 % des données pour l'entraînement et 10 % pour les tests. La première fois que nous expérimentons, nous obtenons une bonne estimation des données d'entraînement. Si nous expérimentons à nouveau, en entraînant le modèle avec une combinaison différente d'échantillons, nous obtenons également un bon résultat, mais les résultats seront différents par rapport à la première expérience. En répétant l'expérience avec une combinaison différente d'échantillons d'apprentissage et de test, les résultats sont relativement proches de l'erreur de généralisation, mais distincts les uns des autres. En répétant le processus, nous obtenons une bonne approximation de l'erreur de généralisation, mais la précision est faible, c'est-à-dire que tous les résultats étaient extrêmement différents les uns des autres. Si nous utilisons moins de points de données pour entraîner le modèle et davantage pour le tester, la précision des performances de généralisation sera moindre, mais le modèle aura une bonne précision. La figure ci-dessus le montre. Toutes nos estimations d'erreur sont relativement proches les unes des autres, mais elles sont plus éloignées des véritables performances de généralisation. Pour pallier ce problème, nous utilisons la validation croisée. L'une des mesures d'évaluation hors échantillon les plus courantes est la validation croisée. Dans cette méthode, l'ensemble de données est divisé en k groupes égaux. Chaque groupe est appelé un pli , par exemple quatre plis. Certains des plis peuvent être utilisés comme kit d'entraînement que nous utilisons pour entraîner le modèle, et les pièces restantes sont utilisées comme kit de test que nous utilisons pour tester le modèle. Par exemple, nous pouvons utiliser trois plis pour l'entraînement, puis utiliser un pli pour les tests. Cette opération est répétée jusqu'à ce que chaque partition soit utilisée à la fois pour l'entraînement et les tests. À la fin, nous utilisons les résultats moyens comme estimation de l'erreur hors échantillon. La métrique d'évaluation dépend du modèle. Par exemple, le R^2.\n",
    "Lisez la vidéo à partir de :4:56 et suivez la transcription4:56\n",
    "Le moyen le plus simple d'appliquer la validation croisée consiste à appeler la fonction cross_val_score, qui effectue plusieurs évaluations hors échantillon. Cette méthode est importée depuis le package de sélection de modèles sklearn. Nous utilisons ensuite la fonction cross_val_score. Le premier paramètre d'entrée est le type de modèle que nous utilisons pour effectuer la validation croisée. Dans cet exemple, nous initialisons le modèle de régression linéaire ou l'objet lr, auquel nous avons transmis la fonction cross_val_score. Les autres paramètres sont x_data, les données de la variable prédictive, et y_data, les données de la variable cible. Nous pouvons gérer le nombre de partitions avec le paramètre cv. Ici, cv=3, ce qui signifie que l'ensemble de données est divisé en trois partitions égales. La fonction renvoie un tableau de scores, un pour chaque partition choisie comme ensemble de test. Nous pouvons faire la moyenne des résultats ensemble pour estimer R^2 hors échantillon en utilisant la fonction de moyenne dans NumPy. Voyons une animation. Voyons le résultat du tableau des scores dans la dernière diapositive. Nous avons d'abord divisé les données en trois parties. Nous utilisons deux plis pour l'entraînement, le pli restant pour les tests. Le modèle produira une sortie, nous utiliserons la sortie pour calculer un score. Dans le cas du R^2, c'est-à-dire le coefficient de détermination, nous allons stocker cette valeur dans un tableau, répéter le processus en utilisant deux fois pour l'entraînement et un fois pour les tests, enregistrer le score, puis utiliser une combinaison différente pour l' entraînement et le pli restant pour les tests. Nous enregistrons le résultat final. La fonction cross_val_score renvoie la valeur du score pour nous indiquer le résultat de la validation croisée. Et si nous voulions un peu plus d'informations ? Et si nous voulions connaître les valeurs prédites réelles fournies par notre modèle avant que les valeurs R^2 ne soient calculées ? Pour ce faire, nous utilisons la fonction cross_val_predict. Les paramètres d'entrée sont exactement les mêmes que ceux de la fonction cross_val_score, mais le résultat est une prédiction. Illustrons le processus. Tout d'abord, nous avons divisé les données en trois parties. Nous utilisons deux plis pour l'entraînement, le pli restant pour les tests. Le modèle produira une sortie et nous la stockerons dans un tableau. Nous allons répéter le processus en utilisant deux volets pour la formation, un pour les tests. Le modèle produit à nouveau une sortie. Enfin, nous utilisons les deux derniers plis pour l'entraînement, puis nous utilisons les données des tests. Ce dernier volet de test produit un résultat. Ces prédictions sont stockées dans un tableau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ced938-2b36-487a-8318-66907254af78",
   "metadata": {},
   "source": [
    "# Évaluation et affinement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de71ae77-a0db-4aef-be68-886e0b6d6a7c",
   "metadata": {},
   "source": [
    "L'évaluation du modèle nous indique comment notre modèle fonctionne dans le monde réel. Dans le module précédent, nous avons parlé de l'évaluation dans l'échantillon. L'évaluation intégrée à l'échantillon nous indique dans quelle mesure notre modèle s'adapte aux données déjà fournies pour l'entraîner. Cela ne nous donne pas d'estimation de la capacité du modèle entraîné à prédire les nouvelles données. La solution consiste à diviser nos données, à utiliser les données de l'échantillon ou les données d'entraînement pour entraîner le modèle. Le reste des données, appelé données de test, est utilisé comme données hors échantillon. Ces données sont ensuite utilisées pour évaluer approximativement les performances du modèle dans le monde réel. La séparation des données en ensembles de formation et de test est un élément important de l'évaluation des modèles. Nous utilisons les données de test pour avoir une idée des performances de notre modèle dans le monde réel. Lorsque nous divisons un ensemble de données, la plus grande partie des données est généralement utilisée pour l' entraînement et une plus petite partie est utilisée pour les tests. Par exemple, nous pouvons utiliser 70 % des données pour la formation. Nous utilisons ensuite 30 % pour les tests. Nous utilisons un ensemble de formation pour créer un modèle et découvrir des relations prédictives. Nous utilisons ensuite un ensemble de tests pour évaluer les performances du modèle. Lorsque nous aurons terminé de tester notre modèle, nous devrions utiliser toutes les données pour l'entraîner. Une fonction populaire du package Scikit-learn pour diviser des ensembles de données est la fonction train_test_split. Cette fonction divise de manière aléatoire un ensemble de données en sous-ensembles d'entraînement et de test à partir de l'extrait de code d'exemple. Cette méthode est importée depuis la validation de sklearn.cross. Le paramètre d'entrée y_data est la variable cible. Dans l'exemple d'évaluation d'une voiture, il s'agirait du prix, et x_data, de la liste des variables prédictives. Dans ce cas, ce sont toutes les autres variables du jeu de données sur les voitures que nous utilisons pour essayer de prédire le prix. La sortie est un tableau, x_train et y_train. Les sous-ensembles pour l'entraînement x_test, et y_test les sous-ensembles pour les tests. Dans ce cas, la taille du test est un pourcentage des données du kit de test. Ici, c'est 30 %. L'état aléatoire est une graine aléatoire pour le découpage aléatoire des ensembles de données. L'erreur de généralisation est une mesure de la capacité de nos données à prédire des données inédites. L'erreur que nous obtenons en utilisant nos données de test est une approximation de cette erreur. Cette figure montre la distribution des valeurs réelles en rouge par rapport aux valeurs prédites à partir d'une régression linéaire en bleu. Nous constatons que les distributions sont quelque peu similaires. Si nous générons le même graphique en utilisant les données de test, nous constatons que les distributions sont relativement différentes. La différence est due à une erreur de généralisation et représente ce que nous voyons dans le monde réel. L'utilisation d'un grand nombre de données pour l'entraînement nous donne un moyen précis de déterminer les performances de notre modèle dans le monde réel, mais la précision des performances sera faible. Clarifions-le à l'aide d'un exemple. Le centre de cet œil de bœuf représente l'erreur de généralisation correcte. Supposons que nous prenions un échantillon aléatoire des données en utilisant 90 % des données pour l'entraînement et 10 % pour les tests. La première fois que nous expérimentons, nous obtenons une bonne estimation des données d'entraînement. Si nous expérimentons à nouveau, en entraînant le modèle avec une combinaison différente d'échantillons, nous obtenons également un bon résultat, mais les résultats seront différents par rapport à la première expérience. En répétant l'expérience avec une combinaison différente d'échantillons d'apprentissage et de test, les résultats sont relativement proches de l'erreur de généralisation, mais distincts les uns des autres. En répétant le processus, nous obtenons une bonne approximation de l'erreur de généralisation, mais la précision est faible, c'est-à-dire que tous les résultats étaient extrêmement différents les uns des autres. Si nous utilisons moins de points de données pour entraîner le modèle et davantage pour le tester, la précision des performances de généralisation sera moindre, mais le modèle aura une bonne précision. La figure ci-dessus le montre. Toutes nos estimations d'erreur sont relativement proches les unes des autres, mais elles sont plus éloignées des véritables performances de généralisation. Pour pallier ce problème, nous utilisons la validation croisée. L'une des mesures d'évaluation hors échantillon les plus courantes est la validation croisée. Dans cette méthode, l'ensemble de données est divisé en k groupes égaux. Chaque groupe est appelé un pli , par exemple quatre plis. Certains des plis peuvent être utilisés comme kit d'entraînement que nous utilisons pour entraîner le modèle, et les pièces restantes sont utilisées comme kit de test que nous utilisons pour tester le modèle. Par exemple, nous pouvons utiliser trois plis pour l'entraînement, puis utiliser un pli pour les tests. Cette opération est répétée jusqu'à ce que chaque partition soit utilisée à la fois pour l'entraînement et les tests. À la fin, nous utilisons les résultats moyens comme estimation de l'erreur hors échantillon. La métrique d'évaluation dépend du modèle. Par exemple, le R^2.\n",
    "Lisez la vidéo à partir de \n",
    "Le moyen le plus simple d'appliquer la validation croisée consiste à appeler la fonction cross_val_score, qui effectue plusieurs évaluations hors échantillon. Cette méthode est importée depuis le package de sélection de modèles sklearn. Nous utilisons ensuite la fonction cross_val_score. Le premier paramètre d'entrée est le type de modèle que nous utilisons pour effectuer la validation croisée. Dans cet exemple, nous initialisons le modèle de régression linéaire ou l'objet lr, auquel nous avons transmis la fonction cross_val_score. Les autres paramètres sont x_data, les données de la variable prédictive, et y_data, les données de la variable cible. Nous pouvons gérer le nombre de partitions avec le paramètre cv. Ici, cv=3, ce qui signifie que l'ensemble de données est divisé en trois partitions égales. La fonction renvoie un tableau de scores, un pour chaque partition choisie comme ensemble de test. Nous pouvons faire la moyenne des résultats ensemble pour estimer R^2 hors échantillon en utilisant la fonction de moyenne dans NumPy. Voyons une animation. Voyons le résultat du tableau des scores dans la dernière diapositive. Nous avons d'abord divisé les données en trois parties. Nous utilisons deux plis pour l'entraînement, le pli restant pour les tests. Le modèle produira une sortie, nous utiliserons la sortie pour calculer un score. Dans le cas du R^2, c'est-à-dire le coefficient de détermination, nous allons stocker cette valeur dans un tableau, répéter le processus en utilisant deux fois pour l'entraînement et un fois pour les tests, enregistrer le score, puis utiliser une combinaison différente pour l' entraînement et le pli restant pour les tests. Nous enregistrons le résultat final. La fonction cross_val_score renvoie la valeur du score pour nous indiquer le résultat de la validation croisée. Et si nous voulions un peu plus d'informations ? Et si nous voulions connaître les valeurs prédites réelles fournies par notre modèle avant que les valeurs R^2 ne soient calculées ? Pour ce faire, nous utilisons la fonction cross_val_predict. Les paramètres d'entrée sont exactement les mêmes que ceux de la fonction cross_val_score, mais le résultat est une prédiction. Illustrons le processus. Tout d'abord, nous avons divisé les données en trois parties. Nous utilisons deux plis pour l'entraînement, le pli restant pour les tests. Le modèle produira une sortie et nous la stockerons dans un tableau. Nous allons répéter le processus en utilisant deux volets pour la formation, un pour les tests. Le modèle produit à nouveau une sortie. Enfin, nous utilisons les deux derniers plis pour l'entraînement, puis nous utilisons les données des tests. Ce dernier volet de test produit un résultat. Ces prédictions sont stockées dans un tableau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71899642-3daa-4b52-b53f-6f43ffc8b65c",
   "metadata": {},
   "source": [
    "# Surajustement, sous-ajustement et sélection de modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0c060-e2d8-4dc8-b457-3342d1e024eb",
   "metadata": {},
   "source": [
    "Si vous vous souvenez, dans le dernier module, nous avons discuté de la régression polynomiale. Dans cette section, nous verrons comment choisir le meilleur ordre polynomial et les problèmes qui se posent lors de la sélection d'un polynôme d'ordre incorrect. Considérez la fonction suivante. Nous supposons que les points d'apprentissage proviennent d' une fonction polynomiale additionnée d'un peu de bruit. L'objectif de la sélection du modèle est de déterminer l'ordre du polynôme afin de fournir la meilleure estimation de la fonction y (x). Si nous essayons d'ajuster la fonction avec une fonction linéaire, la ligne n'est pas suffisamment complexe pour s'adapter aux données. Par conséquent, il y a de nombreuses erreurs. C'est ce que l'on appelle le sous-ajustement, lorsque le modèle est trop simple pour s'adapter aux données. Si nous augmentons l'ordre du polynôme, le modèle s'adapte mieux. Mais le modèle n'est toujours pas assez flexible et présente des défauts d'adaptation. Il s'agit d'un exemple du polynôme du huitième ordre utilisé pour ajuster les données. Nous constatons que le modèle réussit bien à ajuster les données et à estimer la fonction même aux points d'inflexion, en l'augmentant jusqu'à un polynôme d'ordre 16. Le modèle réussit extrêmement bien à suivre le point d'entraînement, mais donne de mauvais résultats pour estimer la fonction. Cela est particulièrement évident lorsqu'il existe peu de données d'entraînement. La fonction estimée oscille et ne suit pas la fonction. C'est ce qu'on appelle le surajustement, lorsque le modèle est trop flexible et s'adapte au bruit plutôt qu'à la fonction. Examinons un graphique de l'erreur quadratique moyenne pour l'ensemble d'apprentissage et de test de polynômes d'ordre différent. L'axe horizontal représente l'ordre du polynôme. L'axe vertical représente l'erreur quadratique moyenne. L'erreur d'apprentissage diminue avec l'ordre du polynôme. L'erreur de test est un meilleur moyen d'estimer l'erreur d'un polynôme. L'erreur diminue jusqu'à ce que le meilleur ordre du polynôme soit déterminé. Ensuite, l'erreur commence à augmenter. Nous sélectionnons la commande qui minimise l'erreur de test. Dans ce cas, il y en avait huit. Tout ce qui se trouve à gauche serait considéré comme sous-adapté. Tout ce qui se trouve sur la droite est trop adapté. Si nous sélectionnons le meilleur ordre du polynôme, nous aurons encore quelques erreurs. Si vous vous souvenez de l' expression originale pour les points d'entraînement, nous voyons un terme de bruit. Ce terme est l'une des raisons de l'erreur. C'est parce que le bruit est aléatoire et que nous ne pouvons pas le prévoir. C'est ce que l'on appelle parfois une erreur irréductible. Il existe également d'autres sources d'erreurs. Par exemple, notre hypothèse polynomiale peut être fausse. Nos points d'échantillonnage peuvent provenir d'une fonction différente. Par exemple, dans ce graphique, les données sont générées à partir d'une onde sinusoïdale. La fonction polynomiale n' ajuste pas correctement l'onde sinusoïdale. Pour les données réelles, il se peut que le modèle soit trop difficile à ajuster ou que nous ne disposions pas du bon type de données pour estimer la fonction. Essayons différents polynômes d'ordre sur les données réelles en utilisant la puissance. Les points rouges représentent les données d'entraînement, les points verts les données de test. Si nous nous contentons d'utiliser la moyenne des données, notre modèle ne fonctionne pas bien. Une fonction linéaire s'adapte mieux aux données. Un modèle du second ordre ressemble à une fonction linéaire. Une fonction du troisième ordre semble également augmenter, comme les deux ordres précédents. Nous voyons ici un polynôme du quatrième ordre. À environ 200 chevaux, le prix prévu diminue soudainement. Cela semble erroné. Utilisons R^2 pour voir si notre hypothèse est correcte. Voici un graphique de la valeur R^2. L'axe horizontal représente les modèles polynomiaux d'ordre. Plus le R^2 est proche de un, plus le modèle est précis. Ici, nous voyons que R^2 est optimal lorsque l'ordre du polynôme est de trois. Le R^2 diminue considérablement lorsque l'ordre passe à quatre, ce qui valide notre hypothèse initiale. Nous pouvons calculer différentes valeurs de R^2 comme suit. Tout d'abord, nous créons une liste vide pour stocker les valeurs. Nous créons une liste contenant différents ordres polynomiaux. Nous parcourons ensuite la liste à l'aide d'une boucle. Nous créons un objet de caractéristique polynomiale avec l'ordre du polynôme comme paramètre. Nous transformons les données d'entraînement et de test en un polynôme à l'aide de la méthode de transformation d'ajustement. Nous ajustons le modèle de régression à l'aide des données de transformation. Nous calculons ensuite le R^2 à l'aide des données de test et le stockons dans le tableau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c44994d-1dbc-428b-b9ca-6777dbed12cd",
   "metadata": {},
   "source": [
    "# Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f73e99-9d3a-49ec-9c63-d133b1cc8617",
   "metadata": {},
   "source": [
    "nous allons parler de la régression des crêtes. La régression des crêtes empêche le surajustement. Dans cette vidéo, nous allons nous concentrer sur la régression polynomiale à des fins de visualisation. Mais le surajustement est également un gros problème lorsque vous avez plusieurs variables ou fonctionnalités indépendantes. Considérons le polynôme du quatrième ordre suivant en orange. Les points bleus sont générés à partir de cette fonction. Nous pouvons utiliser un polynôme d'ordre 10 pour ajuster les données. La fonction estimée en bleu permet d'obtenir une bonne approximation de la fonction vraie. Dans de nombreux cas, les données réelles présentent des valeurs aberrantes. Par exemple, ce point présenté ici ne semble pas provenir de la fonction en orange. Si nous utilisons une fonction polynomiale d'ordre 10 pour ajuster les données, la fonction estimée en bleu est incorrecte et ne constitue pas une bonne estimation de la fonction réelle en orange. Si nous examinons l'expression de la fonction estimée, nous constatons que les coefficients polynomiaux estimés ont une très grande amplitude. Cela est particulièrement évident pour les polynômes d'ordre supérieur. La régression de crête contrôle l'amplitude de ces coefficients polynomiaux en introduisant le paramètre Alpha. Alpha est un paramètre que nous sélectionnons avant d'ajuster ou d'entraîner le modèle. Chaque ligne du tableau suivant représente une valeur croissante d'Alpha. Voyons comment les différentes valeurs d'Alpha modifient le modèle. Ce tableau représente les coefficients polynomiaux pour différentes valeurs d'Alpha. La colonne correspond aux différents coefficients polynomiaux, et les lignes correspondent aux différentes valeurs d'Alpha. À mesure que l'alpha augmente, les paramètres diminuent. Cela est particulièrement évident pour les caractéristiques polynomiales d'ordre supérieur. Mais Alpha doit être sélectionné avec soin. Si Alpha est trop élevé, les coefficients approcheront de zéro et seront sous-ajustés aux données. Si Alpha est nul, le surajustement est évident. Pour un Alpha égal à 0,001, le surajustement commence à s'atténuer. Pour un Alpha égal à 0,01, la fonction estimée suit la fonction réelle. Quand Alpha est égal à un, nous voyons les premiers signes d'un sous-ajustement. La fonction estimée n'est pas suffisamment flexible. À un Alpha égal à 10, nous constatons un sous-ajustement extrême. Il ne suit même pas les deux points. Pour sélectionner Alpha, nous utilisons la validation croisée. Pour faire une prédiction à l'aide de la régression des crêtes, importez une crête à partir des modèles linéaires de Sklearn. Créez un objet arête à l'aide du constructeur. Le paramètre Alpha est l'un des arguments du constructeur. Nous entraînons le modèle en utilisant la méthode d'ajustement. Pour faire une prédiction, nous utilisons la méthode des prédictions. Afin de déterminer le paramètre Alpha, nous utilisons certaines données pour l'entraînement. Nous utilisons un deuxième ensemble appelé données de validation. Ceci est similaire aux données de test, mais il est utilisé pour sélectionner des paramètres tels que Alpha. Nous commençons par une petite valeur d'Alpha. Nous entraînons le modèle, faisons une prédiction à l'aide des données de validation, puis calculons le R^2 et stockons les valeurs. Répétez la valeur pour une valeur Alpha plus élevée. Nous entraînons à nouveau le modèle, faisons une prédiction à l'aide des données de validation, puis calculons le R^2 et stockons les valeurs de R^2. Nous répétons le processus pour une valeur Alpha différente, en entraînant le modèle et en faisant une prédiction. Nous sélectionnons la valeur d'Alpha qui maximise le R^2. Notez que nous pouvons utiliser d'autres métriques pour sélectionner la valeur d'Alpha, comme l'erreur quadratique moyenne. Le problème de surajustement est encore pire si nous avons de nombreuses fonctionnalités. Le graphique suivant montre les différentes valeurs de R^2 sur l'axe vertical. L'axe horizontal représente différentes valeurs pour Alpha. Nous utilisons plusieurs fonctionnalités de notre ensemble de données sur les voitures d'occasion et une fonction polynomiale du second ordre. Les données d'entraînement sont en rouge et les données de validation en bleu. Nous voyons que lorsque la valeur d'Alpha augmente, la valeur de R^2 augmente et converge à environ 0,75. Dans ce cas, nous sélectionnons la valeur maximale d'Alpha car l' exécution de l'expérience pour des valeurs d'Alpha plus élevées n'a que peu d'impact. Inversement, lorsque Alpha augmente, le R^2 des données de test diminue. En effet, le terme Alpha empêche le surajustement. Cela peut améliorer les résultats dans les données invisibles, mais le modèle présente de moins bonnes performances sur les données de test. Consultez le laboratoire pour savoir comment générer ce graphique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422239a6-b0e8-46f0-ac59-e5f17f522648",
   "metadata": {},
   "source": [
    " # Recherche par quadrillage(Grid Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2796cfa-4f48-4785-b7b8-81a42bab9895",
   "metadata": {},
   "source": [
    " Grid Search nous permet de parcourir plusieurs paramètres libres en quelques lignes de code. Les paramètres tels que le terme alpha décrit dans la vidéo précédente ne font pas partie du processus d'ajustement ou d'apprentissage. Ces valeurs sont appelées hyperparamètres.\n",
    "\n",
    "Scikit-learn dispose d'un moyen d'itérer automatiquement ces hyperparamètres à l'aide d'une validation croisée. Cette méthode s'appelle Grid Search. Grid Search utilise le modèle ou les objets que vous souhaitez entraîner et différentes valeurs des hyperparamètres. Il calcule ensuite l'erreur quadratique moyenne ou r au carré pour différentes valeurs d'hyperparamètres, ce qui vous permet de choisir les meilleures valeurs. Laissez les petits cercles représenter les différents hyperparamètres. Nous partons d'une valeur pour les hyperparamètres et nous entraînons le modèle. Nous utilisons différents hyperparamètres pour entraîner le modèle. Nous continuons le processus jusqu'à ce que nous ayons épuisé les différentes valeurs de paramètres libres. Chaque modèle produit une erreur, nous sélectionnons l'hyperparamètre qui minimise l'erreur. Pour sélectionner l'hyperparamètre, nous avons divisé notre ensemble de données en trois parties : l'ensemble d' apprentissage, le jeu de validation et le jeu de test. Nous entraînons le modèle pour différents hyperparamètres, nous utilisons l'erreur quadratique r ou l' erreur quadratique moyenne pour chaque modèle. Nous sélectionnons l'hyperparamètre qui minimise l'erreur quadratique moyenne ou maximise le r au carré sur l'ensemble de validation. Nous testons enfin les performances de notre modèle à l'aide des données de test. Il s'agit de la page Web Scikit-learn où les paramètres du constructeur d'objets sont donnés. Il est à noter que les attributs d'un objet sont également appelés paramètres. Nous ne ferons pas de distinction même si certaines options ne sont pas des hyperparamètres en soi. Dans ce module, nous allons nous concentrer sur l'hyperparamètre alpha et le paramètre de normalisation. La valeur de votre recherche par grille est une liste Python contenant un dictionnaire Python, la clé étant le nom du paramètre libre. La valeur du dictionnaire correspond aux différentes valeurs du paramètre libre. Cela peut être visualisé sous la forme d'un tableau avec différentes valeurs de paramètres libres, nous avons également l'objet ou le modèle. La recherche par grille utilise la méthode de notation, dans ce cas r au carré, le nombre de plis, le modèle ou l'objet et les valeurs des paramètres libres. Certaines des sorties incluent les différents scores pour différentes valeurs de paramètres libres. Dans ce cas, le r est au carré avec les valeurs des paramètres libres qui ont le meilleur score. Tout d' abord, nous importons les bibliothèques dont nous avons besoin, notamment Grid Search CV, le dictionnaire des valeurs des paramètres. Nous créons un objet ou un modèle de régression de crête, puis nous créons un objet GridSearchCV. Les entrées sont l'objet de régression de crête, les valeurs des paramètres et le nombre de plis. Nous utiliserons r au carré, c'est la méthode de notation par défaut. Nous ajustons l'objet, nous pouvons trouver les meilleures valeurs pour les paramètres libres en utilisant le meilleur estimateur d'attributs. Nous pouvons également obtenir des informations telles que le score moyen sur les données de validation en utilisant l'attribut CV result. L'un des avantages de Grid Search est la rapidité avec laquelle nous pouvons tester plusieurs paramètres.\n",
    "\n",
    "Par exemple, la régression par crête permet de normaliser les données. Pour voir comment standardiser, reportez-vous au module 4. Le terme alpha est le premier élément du dictionnaire, le second est l'option de normalisation. La clé est le nom du paramètre, la valeur correspond aux différentes options dans ce cas, car nous pouvons normaliser les données ou non. Les valeurs sont vraies ou fausses, respectivement. Le dictionnaire est une table ou une grille contenant deux valeurs différentes. Comme précédemment, nous avons besoin de l'objet ou du modèle de régression de crête. La procédure est similaire, sauf que nous avons une table ou une grille de valeurs de paramètres différentes. Le résultat est le score pour toutes les différentes combinaisons de valeurs de paramètres, le code est également similaire. Le dictionnaire contient les différentes valeurs de paramètres libres, nous pouvons trouver la meilleure valeur pour les paramètres libres. Les scores obtenus pour les différents paramètres libres sont stockés dans ce dictionnaire, Grid1.cv_results_. Nous pouvons imprimer le score pour les différentes valeurs de paramètres libres. Les valeurs des paramètres sont stockées comme indiqué ici. Consultez les ateliers du cours pour plus d'exemples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3948823-dfe9-4a10-be6a-b7c6f96ba7b2",
   "metadata": {},
   "source": [
    "# Résumé de la leçonStatut : Traduit automatiquement de Anglais\n",
    "Traduit automatiquement de Anglais\n",
    "Nous vous félicitons ! Vous avez terminé cette leçon. À ce stade du cours, vous savez\n",
    "\n",
    "Comment diviser vos données à l'aide de la méthode train_test_split() en ensembles d'entraînement et de test. Vous utilisez l'ensemble d'entraînement pour former un modèle, découvrir les relations prédictives possibles, puis vous utilisez l'ensemble de test pour tester votre modèle et évaluer ses performances.\n",
    "\n",
    "Comment utiliser l'erreur de généralisation pour mesurer la capacité de vos données à prédire des données inédites.\n",
    "\n",
    "Comment utiliser la validation croisée en divisant les données en plis dont une partie sert d'ensemble d'entraînement, que nous utilisons pour entraîner le modèle, et l'autre partie sert d'ensemble de test, que nous utilisons pour tester le modèle. Vous itérez à travers les plis jusqu'à ce que vous utilisiez chaque partition pour l'entraînement et le test. À la fin, vous faites la moyenne des résultats en tant qu'estimation de l'erreur hors échantillon.\n",
    "\n",
    "Comment choisir le meilleur ordre polynomial et les problèmes qui surviennent lors de la sélection d'un polynôme de mauvais ordre en analysant les modèles qui sous-adaptent et suradaptent vos données.\n",
    "\n",
    "Sélectionnez le meilleur ordre d'un polynôme pour ajuster vos données en minimisant l'erreur de test à l'aide d'un graphique comparant l'erreur quadratique moyenne à l'ordre des polynômes ajustés.\n",
    "\n",
    "Vous devez utiliser la régression ridge lorsqu'il existe une forte relation entre les variables indépendantes.\n",
    "\n",
    "La régression ridge permet d'éviter l'ajustement excessif.\n",
    "\n",
    "La régression ridge contrôle l'ampleur des coefficients polynomiaux en introduisant un hyperparamètre, alpha.\n",
    "\n",
    "Pour déterminer alpha, vous divisez vos données en données d'apprentissage et de validation. En commençant par une petite valeur pour alpha, vous entraînez le modèle, faites une prédiction à l'aide des données de validation, puis calculez le R au carré et enregistrez les valeurs. Répétez l'opération pour une valeur alpha plus élevée. Répétez le processus pour différentes valeurs d'alpha, en entraînant le modèle et en effectuant une prédiction. Vous sélectionnez la valeur d'alpha qui maximise le R-carré.\n",
    "\n",
    "Cette recherche en grille vous permet de parcourir plusieurs hyperparamètres à l'aide de la bibliothèque Scikit-learn, qui itère sur ces paramètres à l'aide de la validation croisée. Sur la base des résultats de la méthode de recherche en grille, vous sélectionnez les valeurs optimales des hyperparamètres.\n",
    "\n",
    "La méthode GridSearchCV() prend en argument un dictionnaire dont la clé est le nom de l'hyperparamètre et les valeurs sont les valeurs de l'hyperparamètre sur lesquelles vous souhaitez itérer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b72c5b-ff89-4e27-b2fb-16a4c817e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTURldmVsb3BlclNraWxsc05ldHdvcmstREEwMTAxRU4tQ291cnNlcmEvbGFicy8yMDA1NDIuMDgxX001X0NoZWF0X1NoZWV0Lm1kIiwidG9vbF90eXBlIjoiaW5zdHJ1Y3Rpb25hbC1sYWIiLCJhZG1pbiI6ZmFsc2UsImlhdCI6MTcxMTYzODYxMn0.8eaKLWscOEzw-SqrXX59bA5Ir81T9oIKzjd2qa5QEfE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e1008-d767-4823-89f0-376517f378db",
   "metadata": {},
   "source": [
    "# aperçu du projet pratique la prévision des prix sur l'assurance médicale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563ab52-6883-40f3-a828-9f422fa05db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTURldmVsb3BlclNraWxsc05ldHdvcmstREEwMTAxRU4tQ291cnNlcmEvbGFicy92NC9wcmFjdGljZV9wcm9qZWN0X3dyaXRldXAubWQiLCJ0b29sX3R5cGUiOiJpbnN0cnVjdGlvbmFsLWxhYiIsImFkbWluIjpmYWxzZSwiaWF0IjoxNzExNjM4NjExfQ.XTF7AWjnGt6SlkPD9hRVfr0KewEW1hMcRR9MqPiCZHw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f413fd39-945c-424f-ba28-49df7879d285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
